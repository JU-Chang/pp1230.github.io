---
layout: post
title: 使用Octave的梯度下降示例
description: 机器学习视频之梯度下降练习一
keywords: gradient descent
---


## 一个参数的线性回归
## 1.描绘数据散点图
{%highlight matlab%}
data = load('ex1data1.txt'); % read comma separated 
X = data(:, 1); 
y = data(:, 2);
m = length(y); % number of training example
plot(x, y, 'rx', 'MarkerSize', 10); % Plot the data 
ylabel('Profit in $10,000s'); % Set the y−axis label 
xlabel('Population of City in 10,000s'); % Set the x−axis label 
#{%endhighlight matlab%}

doc eg:

'x'  cross
'r'  Red

plot (b, "*", "markersize", 10)


This command will plot the data in the variable 'b', with points 

displayed as '*' and a marker size of 10.

<img src="https://pp1230.github.io/static/images/sdt.png" width = "500" alt="dcxff" />y
## 2.使用梯度下降计算参数
    
{%highlight matlab%}
    %梯度下降算法函数，储存在.m文件中
    function theta = gradientDescent1(x,y,m,alpha,iteraNum)
    theta = [0;0];
j    for i=1:iteraNum
    j = x*theta- y;
    theta(1) = theta(1) - alpha*(1/m)*sum(j.*x(:,1));
    theta(2) = theta(2) - alpha*(1/m)*sum(j.*x(:,2));
    end;
    
    %调用画图函数
    theta = gradientDescent1(x,y,m,alpha,1500);
    hold on;
    plot(x(:,2),x*theta,'-')
    legend("Training Data","Linear Regression") %图例
{%endhighlight matlab%}

<img src="https://pp1230.github.io/static/images/tdxj.png" width = "500" alt="dcxff" />
## 3.绘制三维和等高图像uo
{%highlight matlab%}
% Fill out J vals 
for i = 1:length(theta0 vals) 
for j = 1:length(theta1 vals) 
t = [theta0 vals(i); theta1 vals(j)]; 
J vals(i,j) = computeCost(x, y, t); 
end 
end

% Because of the way meshgrids work in the surf command, we need to 
% transpose J_vals before calling surf, or else the axes will be flipped
J_vals = J_vals';
% Surface plot

figure;

surf(theta0_vals, theta1_vals, J_vals)
xlabel('\theta_0'); ylabel('\theta_1');

% Contour plot
figure;
d% Plot J_vals as 15 contours spaced logarithmically between 0.01 and 100
contour(theta0_vals, theta1_vals, J_vals, logspace(-2, 3, 20))
xlabel('\theta_0'); ylabel('\theta_1');
hold on;
plot(theta(1), theta(2), 'rx', 'MarkerSize', 10, 'LineWidth', 2);

{%endhighlight matlab%}

<img src="https://pp1230.github.io/static/images/swt.png" width = "800" alt="dcxff" />

## 4.实践
### 4.1梯度下降与迭代次数的关系

例子1：
使用线性回归和梯度下降拟合测试数据。随着迭代次数的增加，参数离极小值越来越近。

<img src="https://pp1230.github.io/static/images/lrgd.png" width = "500" alt="dcxff" />

图为步长为0.001，迭代5000步后的图像。

例子2：
多元线性回归，面积、房间数和房屋价格的关系

{%highlight matlab%}
data = load("ex1data2.txt");
data1 = data(:,1)/1000;
data2 = data(:,2);
data3 = data(:,3)/100000;
m = length(data(:,1));
t = [0;0;0];
% Fill out J_vals
for i = 1:100
	  t = gradientDescent1([ones(m,1),data1,data2],data3,t,0.1,1);
	  J_vals(i) = computeCost([ones(m,1),data1,data2],
data3,t);
g    end;
plot(linspace(1,100,100),J_vals);

t =
0.383085
1.350005
0.093650
{%endhighlight matlab%}
随着迭代次数增加，cost不断减少。

<img src="https://pp1230.github.io/static/images/costf.png" width = "500" alt="dcxff" />

### 4.2梯度下降与代价函数的关系
代价函数中的h(x)与图像形态有关。根据图像的形态可选择线性回归或非线性回归，根据特征的数量选择一元或多元回归，都可以使用梯度下降法。

例子2：多元线性回归，房价与面积、房间数的关系
{%highlight matlab%}
plot3(data1,data2,data3,".")
hold on; 
x1 = linspace(0,5,100);
y1 = linspace(1,5,100);
plot3(x1,y1,[ones(100,1),x1',y1']*t);
legend("real data","linear regression");
xlabel("Area(1000m^2)");ylabel("Room Number");zlabel("Price($100,000)");
{%endhighlight matlab%}

<img src="https://pp1230.github.io/static/images/mlr.png" width = "500" alt="dcxff" />

### 4.3特征缩放
在数据集中，数据的单位往往不同，数据的范围也差距很大。将数据缩放到相同的范围，一般为[0,1]或[-1,1]使得梯度下降法更容易收敛，还可以统一坐标轴的度量单位，更加准确表达函数图形。在4.2和4.1中，将特征进行了粗略的缩放，也可以达到相同的效果。根据特征缩放公式，下面将特征范围都缩放到[0,1]。

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <msup>
    <mi>x</mi>
    <mo>&#x2032;</mo>
  </msup>
  <mo>=</mo>
  <mfrac>
    <mrow>
      <mi>x</mi>
      <mo>&#x2212;<!-- − --></mo>
      <mi>m</mi>
      <mi>i</mi>
      <mi>n</mi>
      <mo stretchy="false">(</mo>
      <mi>x</mi>
      <mo stretchy="false">)</mo>
    </mrow>
    <mrow>
      <mi>m</mi>
      <mi>a</mi>      <mi>x</mi>
      <mo stretchy="false">(</mo>
      <mi>x</mi>
      <mo stretchy="false">)</mo>
      <mo>&#x2212;<!-- − --></mo>
      <mi>m</mi>
      <mi>i</mi>
      <mi>n</mi>
      <mo stretchy="false">(</mo>
      <mi>x</mi>
      <mo stretchy="false">)</mo>
    </mrow>
  </mfrac>
</math>

{%highlight matlab%}
data = load("ex1data2.txt");
scala1 = max(data(:,1))-min(data(:,1));
scala2 = max(data(:,2))-min(data(:,2));
scala3 = max(data(:,3))-min(data(:,3));
min1 = min(data(:,1));
min2 = min(data(:,2));
min3 = min(data(:,3));
data1 = (data(:,1).-min1)/scala1;
data2 = (data(:,2).-min2)/scala2;
data3 = (data(:,3).-min3)/scala3;
m = length(data(:,1));
t = [0;0;0];
% Fill out J_vals
for i = 1:6000
	  t = gradientDescent1([ones(m,1),data1,data2],data3,t,0.01,1);
	  J_vals(i) = computeCost([ones(m,1),data1,data2],
data3,t);
    end;
plot(linspace(1,6000,6000),J_vals);

figure
x1 = linspace(0,1,100);
y1 = linspace(0,1,100);
plot3(data1,data2,data3,"r.")
hold on; 
%plot3(x1,y1,[ones(100,1),x1',y1']*t);
v = zeros(100);
for i=1:100
	for j = 1:100
		v(i,j) = [1,x1(i),y1(j)]*t;
end
end
surf(x1,y1,v); 
legend("real data","linear regression");
xlabel("Area");ylabel("Room Number");zlabel("Price");
{%endhighlight matlab%}

<img src="https://pp1230.github.io/static/images/lr3d.png" width = "500" alt="dcxff" />

<img src="https://pp1230.github.io/static/images/cost3d.png" width = "500" alt="dcxff" />
