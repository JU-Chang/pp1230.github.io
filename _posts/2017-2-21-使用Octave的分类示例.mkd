---
layout: post
title: Octave分类示例
description: 机器学习视频之分类练习一,包含了逻辑回归解决分类问题。
keywords: gradient descent
---

> 使用了机器学习视频的练习数据集[ex2data](https://github.com/pp1230/pp1230.github.io/tree/master/static/pdf)

## 1.逻辑回归分类

### 1.1使用线性函数的逻辑回归二分类问题

逻辑回归的核心公式是sigmoid函数，使用线性函数可以线性划分类别。使用Octave的fminunc方法可以自动选取最优化方法求解，
不需要自己再编写梯度下降或共轭梯度等复杂的优化算法。我们只需要写出代价函数和导数即可。

{%highlight matlab%}
data = load("ex2data1.txt");
m = length(data(:,1));
class1 = [];
class2 = [];
for i = 1:m
	x = data(i,:);
	if x(3) == 0
	class1 = [class1;x(1),x(2)];
	else 
	class2 = [class2;x(1),x(2)];
	hold on;
	end
end
plot(class1(:,1),class1(:,2),"bo",'LineWidth', 1.5,"markersize",10);
hold on;
plot(class2(:,1),class2(:,2),"rs",'LineWidth', 1.5,"markersize",10);

inittheta = [0;0;0];
x = [ones(m,1),data(:,1),data(:,2)];
y = data(:,3);
options = optimset("GradObj","on","MaxIter",1000);
[theta,cost] = fminunc(@(t)logisticCostFunction(t,x,y),inittheta,options);
x1 = linspace(30,100,100);
y1 = -(x1*theta(2) + theta(1))/theta(3);
plot(x1,y1,"LineWidth", 1.5);
xlabel("x1");
ylabel("x2");
legend("class1","class2","decision boundary");
{%endhighlight matlab%}

<img src="https://pp1230.github.io/static/images/classpic1.png" width = "500" alt="dcxff" />

### 1.2使用非线性函数的逻辑回归二分类问题

逻辑回归的sigmoid函数中使用非线性函数作为变量，可以更好的拟合复杂的分类问题。高阶函数使用的关键点有：高阶函数的形式和次方；高阶函数的正则化。
因为在函数拟合中，可能出现过拟合现象，为了使函数能够良好地拟合真实的数据，我们使用惩罚函数地方法来防止过拟合和现象的发生。本实验使用的高阶函数为：

<img src="https://pp1230.github.io/static/images/mapFeature.png" width = "200" alt="dcxff" />

{%highlight matlab%}
data = load("ex2data2.txt");
m = length(data(:,1));
class1 = [];
class2 = [];
for i = 1:m
	x = data(i,:);
	if x(3) == 0
	class1 = [class1;x(1),x(2)];
	else 
	class2 = [class2;x(1),x(2)];
	hold on;
	end
end
plot(class1(:,1),class1(:,2),"ko","markerfacecolor","y",'LineWidth', 1,"markersize",8);
hold on;
plot(class2(:,1),class2(:,2),"rs",'LineWidth', 1.5,"markersize",8);
%高阶函数自定义计算方法
X = regMapFeature(data(:,1),data(:,2),6);
y = data(:,3);
inittheta = zeros(size(X,2),1);
%罚函数变量
lambda = 1;
[initcost, grad] = regLogisticCostFunction(inittheta, X, y, lambda);
options = optimset("GradObj","on","MaxIter",400);
[theta,cost,exit_flag] = ...
	fminunc(@(t)regLogisticCostFunction(t,X,y,lambda),inittheta,options);
x1 = linspace(-1,1.2,50);
y1 = linspace(-1,1.2,50);
z1 = zeros(length(x1),length(y1));
for i=1:length(x1)
	for j=1:length(y1)
		 z1(j,i) = regMapFeature(x1(i),y1(j),6)*theta;
	end
end
%边界绘制
contour(x1,y1,z1,[0,0],"LineWidth",2);
xlabel("x1");
ylabel("x2");
legend("class1","class2","decision boundary");
{%endhighlight matlab%}

<img src="https://pp1230.github.io/static/images/regclass1.png" width = "500" alt="dcxff" />

通过改变lambda,函数的拟合度会发生改变。令lambda = 0，函数图象过拟合：

<img src="https://pp1230.github.io/static/images/regclassla.png" width = "500" alt="dcxff" />

通过改变高阶函数的维度，函数图像也会发生改变。让高阶函数从28维变为6维，函数图像变化如下：

<img src="https://pp1230.github.io/static/images/regclassdim.png" width = "500" alt="dcxff" />

### 1.3逻辑回归的多分类问题

逻辑回归本身用来处理二分类问题，将 **theta'x** 的值投射到[0,1]区间上。处理多分类问题时，应将其划归为二分类问题，也就是oneVsAll。
本例展示了逻辑回归处理手写识别的效果，使用了线性函数，特征维数为400。若使用非线性函数，那么维数将指数增长，效果不好。

{%highlight matlab%}
load("ex3data1.mat");
m = size(X,1);
pics = randperm(m);
test = X(pics(1:100),:);
displayData(test);

lambda = 0.1;
classNumber = 10;
alltheta = multiClassFunction(X,y,10,lambda);
pred = predictOneVsAll(alltheta, X);
fprintf('\nTraining Set Accuracy: %f\n', mean(double(pred == y)) * 100);

testdata = [ones(100,1),test];
result = testdata*alltheta';
final = zeros(100,1);
matrix = zeros(10);
{%endhighlight matlab%}

将多分类问题转换为二分类问题，每个类别和其他类别进行一次逻辑回归分类。

{%highlight matlab%}
function allTheta = multiClassFunction(x,y,classNumber,lambda)
m = size(x,2);
n = size(x,1);
inittheta = zeros(m+1,1);
x = [ones(n,1),x];
options = optimset('GradObj', 'on', 'MaxIter', 80);
for c = 1:classNumber
    [allTheta(c,:)] =fminunc (@(t)(logCostFunction(t, x, (y == c), lambda)),inittheta, options);
end
{%endhighlight matlab%}

100个测试数据

<img src="https://pp1230.github.io/static/images/multilognum.png" width = "500" alt="dcxff" />

总体精度和测试数据结果

{%highlight c%}
Training Set Accuracy: 94.500000
>> matrix
matrix =
2   6   1   5   1   7   9   1   1   5
4   8   6   2   9   6   8   9   7   0
9   4   0   9   0   2   8   7   5   5
7   0   4   2   9   6   8   5   1   2
7   6   7   5   0   9   1   0   1   8
3   8   2   2   3   8   4   2   5   4
2   1   7   9   7   0   7   5   9   3
9   5   2   2   9   2   5   8   7   8
4   5   0   6   0   8   4   9   8   1
7   4   0   6   3   8   6   5   4   6
{%endhighlight%}
