---
layout: post
title: 词向量和语言模型(一)
description: 
    将词用  '词向量'  的方式表示可谓是将 Deep Learning 算法引入 NLP 领域的一个核心技术。大多数宣称用了 Deep Learning 的论文，其中往往也用了词向量。自然语言理解的问题要转化为机器学习的问题，第一步肯定是要找一种方法把这些符号数学化。</br>
    Deep Learning 中一般用到的词向量并不是刚才提到的用 One-hot Representation 表示的那种很长很长的词向量，而是用 Distributed Representation表示的一种低维实数向量。这种向量一般长成这个样子：[0.792, −0.177, −0.107, 0.109, −0.542, …]。维度以 50 维和 100 维比较常见。这种向量的表示不是唯一的，后文会提到目前计算出这种向量的主流方法。</br>
    Distributed representation 最大的贡献就是让相关或者相似的词，在距离上更接近了。要介绍词向量是怎么训练得到的，就不得不提到语言模型。到目前为止我了解到的所有训练方法都是在训练语言模型的同时，顺便得到词向量的。这也比较容易理解，要从一段无标注的自然文本中学习出一些东西，无非就是统计出词频、词的共现、词的搭配之类的信息。
keywords: 词向量
---

##1.什么是词向量
* **Deep Learning** 中一般用到的**词向量**并不是刚才提到的用 One-hot Representation 表示的那种很长很长的词向量，而是用 Distributed Representation（不知道这个应该怎么翻译，因为还存在一种叫“Distributional Representation”的表示方法，又是另一个不同的概念）表示的一种低维实数向量。这种向量一般长成这个样子：[0.792, −0.177, −0.107, 0.109, −0.542, …]。维度以 50 维和 100 维比较常见。这种向量的表示不是唯一的，后文会提到目前计算出这种向量的主流方法。

* **Distributed representation** 最大的贡献就是让相关或者相似的词，在距离上更接近了。向量的距离可以用最传统的欧氏距离来衡量，也可以用 cos 夹角来衡量。用这种方式表示的向量，“麦克”和“话筒”的距离会远远小于“麦克”和“天气”。可能理想情况下“麦克”和“话筒”的表示应该是完全一样的，但是由于有些人会把英文名“迈克”也写成“麦克”，导致“麦克”一词带上了一些人名的语义，因此不会和“话筒”完全一致。

##2.词向量的训练
要介绍词向量是怎么训练得到的，就不得不提到**语言模型**。到目前为止我了解到的所有训练方法都是在训练语言模型的同时，顺便得到词向量的。

* 这也比较容易理解，要从一段无标注的自然文本中学习出一些东西，无非就是统计出词频、词的共现、词的搭配之类的信息。而要从自然文本中统计并建立一个语言模型，无疑是要求最为精确的一个任务（也不排除以后有人创造出更好更有用的方法）。既然构建语言模型这一任务要求这么高，其中必然也需要对语言进行更精细的统计和分析，同时也会需要更好的模型，更大的数据来支撑。目前最好的词向量都来自于此，也就不难理解了。

* 这里介绍的工作均为从大量未标注的普通文本数据中无监督地学习出词向量（语言模型本来就是基于这个想法而来的），可以猜测，如果用上了有标注的语料，训练词向量的方法肯定会更多。不过视目前的语料规模，还是使用未标注语料的方法靠谱一些。
词向量的训练最经典的有 3 个工作，C&W 2008、M&H 2008、Mikolov 2010。当然在说这些工作之前，不得不介绍一下这一系列中 Bengio 的经典之作。

###2.0知识回顾

####2.0.1语言模型简介
语言模型形式化的描述就是给定一个字符串，看它是自然语言的概率 P(w1,w2,…,wt)。w1 到 wt 依次表示这句话中的各个词。有个很简单的推论是：
{%highlight c%}
P(w1,w2,......,wt) = P(w1) * P(w2|w1) * P(w3|w1,w2) * ...... * P(wt|w1,w2,......,wt-1)
{%endhighlight%}常用的语言模型都是在近似地求P(wt|w1,w2,…,wt−1).比如**n-gram模型**就是用P(wt|wt−n+1,…,wt−1)近似表示前者。

####2.0.2N-Gram模型

* **N-Gram**是大词汇连续语音识别中常用的一种语言模型，对中文而言，我们称之为汉语语言模型(CLM, Chinese Language Model)。汉语语言模型利用上下文中相邻词间的搭配信息，在需要把连续无空格的拼音、笔划，或代表字母或笔划的数字，转换成汉字串(即句子)时，可以计算出具有最大概率的句子，从而实现到汉字的自动转换，无需用户手动选择，避开了许多汉字对应一个相同的拼音(或笔划串，或数字串)的重码问题。<br>
该模型基于这样一种假设，第n个词的出现只与前面N-1个词相关，而与其它任何词都不相关，整句的概率就是各个词出现概率的乘积。这些概率可以通过直接从语料中统计N个词同时出现的次数得到。常用的是二元的Bi-Gram和三元的Tri-Gram。
在介绍N-gram模型之前，让我们先来做个香农游戏（Shannon Game）。我们给定一个词，然后猜测下一个词是什么。当我说“艳照门”这个词时，你想到下一个词是什么呢？我想大家很有可能会想到“陈冠希”，基本上不会有人会想到“陈志杰”吧。N-gram模型的主要思想就是这样的。<br>
对于一个句子T，我们怎么算它出现的概率呢？假设T是由词序列W1,W2,W3,…Wn组成的，那么
{%highlight c%}
P(T)=P(W1W2W3Wn)=P(W1)P(W2|W1)P(W3|W1W2)…P(Wn|W1W2…Wn-1)
{%endhighlight%}

* **条件概率知识**：
<br>**定理1**<br>
设A，B 是两个事件，且A不是不可能事件，则称`P(B|A)=P(AB)/P(A)`为在事件A发生的条件下，事件B发生的条件概率。一般地，  ，且它满足以下三条件：<br>
（1）非负性；（2）规范性；（3）可列可加性。<br>
**定理2**<br>
设E 为随机试验，Ω 为样本空间，A，B 为任意两个事件，设P(A)>0，称 `P(B|A)=P(AB)/P(A)为在`“事件A 发生”的条件下事件B 的条件概率。
上述乘法公式可推广到任意有穷多个事件时的情况。<br>
设A1，A2，…An为任意n 个事件（n≥2）且P（A1A2…An-1）>0，则`P（A1A2…An）=P（A1）P（A2|A1）…P（An|A1A2…An-1）`但是这种方法存在两个致命的缺陷：一个缺陷是参数空间过大，不可能实用化；另外一个缺陷是数据稀疏严重。
为了解决这个问题，我们引入了马尔科夫假设：一个词的出现仅仅依赖于它前面出现的有限的一个或者几个词。
如果一个词的出现仅依赖于它前面出现的一个词，那么我们就称之为bigram。即
{%highlight c%}
P(T) = P(W1W2W3...Wn)=P(W1)P(W2|W1)P(W3|W1W2)...P(Wn|W1W2...Wn-1)
 = P(W1)P(W2|W1)P(W3|W2)...P(Wn|Wn-1){%endhighlight%}
* 如果一个词的出现仅依赖于它前面出现的两个词，那么我们就称之为trigram。
在实践中用的最多的就是bigram和trigram了，而且效果很不错。高于四元的用的很少，因为训练它需要更庞大的语料，而且数据稀疏严重，时间复杂度高，精度却提高的不多。<br>
那么我们怎么得到P(Wn|W1W2…Wn-1)呢？一种简单的估计方法就是**最大似然估计**(Maximum Likelihood Estimate）了。即
{%highlight c%}
P(Wn|W1W2...Wn-1) = (C(W1 W2...Wn))/(C(W1 W2...Wn-1))
{%endhighlight%}
剩下的工作就是在训练语料库中数数儿了，即统计序列C(W1 W2…Wn) 出现的次数和C(W1 W2…Wn-1)出现的次数。

下面我们用bigram举个例子。假设语料库总词数为13,748
![ngrame](../../../static/images/ngram.gif)

计算结果：
P(I want to eat Chinese food)
=P(I)*P(want|I)*P(to|want)*P(eat|to)*P(Chinese|eat)*P(food|Chinese)
=0.25*1087/3437*786/1215*860/3256*19/938*120/213
=0.000154171

了解了噪声信道模型和N-gram模型的思想之后，其实我们自己就能实现一个音词转换系统了，它是整句智能输入法的核心，其实我们不难猜到，搜狗拼音和微软拼音的主要思想就是N-gram模型的，不过在里面多加入了一些语言学规则而已。

##3.对词向量训练的不同方法

###3.1Bengio 的经典之作
**A Neural Probabilistic Language Model**<br>
author:Yoshua Bengio<br>
We propose to fight thecurse of dimensionality by learning a distributed representation for words which allows each
training sentence to inform the model about an exponential number of semantically neighboring
sentences.
        
        
        
        
##参考文献资料：
ngram模型：[CSDN博客网址](http://blog.csdn.net/lengyuhong/article/details/6022053)

DeepLearning in NLP:[licstar的博客](http://licstar.net/archives/328)

Yoshua Bengio, Rejean Ducharme, Pascal Vincent, and Christian Jauvin. A neural probabilistic language model. Journal of Machine Learning Research (JMLR), 3:1137–1155, 2003. [PDF](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)

Ronan Collobert, Jason Weston, Léon Bottou, Michael Karlen, Koray Kavukcuoglu and Pavel Kuksa. Natural Language Processing (Almost) from Scratch. Journal of Machine Learning Research (JMLR), 12:2493-2537, 2011. [PDF](http://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf)

Andriy Mnih & Geoffrey Hinton. Three new graphical models for statistical language modelling. International Conference on Machine Learning (ICML). 2007. [PDF](http://www.cs.utoronto.ca/~hinton/absps/threenew.pdf)
Andriy Mnih & Geoffrey Hinton. A scalable hierarchical distributed language model. The Conference on Neural Information Processing Systems (NIPS) (pp. 1081–1088). 2008. [PDF](http://www.cs.utoronto.ca/~hinton/absps/andriytree.pdf)

Mikolov Tomáš. Statistical Language Models based on Neural Networks. PhD thesis, Brno University of Technology. 2012. [PDF](http://www.fit.vutbr.cz/~imikolov/rnnlm/thesis.pdf)

Turian Joseph, Lev Ratinov, and Yoshua Bengio. Word representations: a simple and general method for semi-supervised learning. Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL). 2010. [PDF](http://www.aclweb.org/anthology-new/P/P10/P10-1040.pdf)

Eric Huang, Richard Socher, Christopher Manning and Andrew Ng. Improving word representations via global context and multiple word prototypes. Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1. 2012. [PDF](http://www-nlp.stanford.edu/pubs/HuangACL12.pdf)

Mikolov, Tomas, Wen-tau Yih, and Geoffrey Zweig. Linguistic regularities in continuous space word representations. Proceedings of NAACL-HLT. 2013. [PDF](https://www.aclweb.org/anthology/N/N13/N13-1090.pdf)
