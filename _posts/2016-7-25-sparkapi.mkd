---
layout: post
title: Spark Api
description: Spark Api 学习
keywords: Spark,rdd
---

SPARK API
=============

> Spark Api Docs : <http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.package>

SparkContext
-------------

* def
**parallelize[T] (seq: Seq[T], numSlices: Int = defaultParallelism(分组参数，可以为空))(implicit arg0: ClassTag[T]): RDD[T]**

    Distribute a local Scala collection to form an RDD.

    **Note**:
avoid using parallelize(Seq()) to create an empty RDD. Consider emptyRDD for an RDD with no partitions, or parallelize(Seq[T]()) for an RDD of T with empty partitions.
,
Parallelize acts lazily. If seq is a mutable collection and is altered after the call to parallelize and before the first action on the RDD, the resultant RDD will reflect the modified collection. Pass a copy of the argument to avoid this.

* def
**textFile(path: String, minPartitions: Int = defaultMinPartitions): RDD[String]**
    Read a text file from HDFS, a local file system (available on all nodes), or any Hadoop-supported file system URI, and return it as an RDD of Strings.

{%highlight scala%}
//从本地文件初始化RDD
scala> val rdd2 = sc.textFile("data.txt")
rdd2: org.apache.spark.rdd.RDD[String] = data.txt MapPartitionsRDD[3] at textFile at <console>:27

scala> rdd2.toArray
res2: Array[String] = Array(1, 11, 222, 3333, 44444)

{%endhighlight%}


RDD
----------


A Resilient Distributed Dataset (RDD), the basic abstraction in Spark. Represents an immutable, partitioned collection of elements that can be operated on in parallel. This class contains the basic operations available on all RDDs, such as map, filter, and persist. In addition, org.apache.spark.rdd.**PairRDDFunctions** contains operations available only on RDDs of key-value pairs, such as groupByKey and join; org.apache.spark.rdd.**DoubleRDDFunctions** contains operations available only on RDDs of Doubles; and org.apache.spark.rdd.**SequenceFileRDDFunctions** contains operations available on RDDs that can be saved as SequenceFiles. All operations are automatically available on any RDD of the right type (e.g. RDD[(Int, Int)] through implicit.

RDD是Spark的基本抽象类，包含了可以分布式运算的集合元素。RDD类包含了基本的map filtet persist方法，当RDD是键值对和Double时有额外的方法。


* def
**++(other: RDD[T]): RDD[T]**

    Return the union of this RDD and another one(合并两个RDD). Any identical elements will appear multiple times (use .distinct() to eliminate them).

* 
def
**aggregate[U] (zeroValue : U)(seqOp: (U, T) ⇒ U, combOp: (U, U) ⇒ U)(implicit arg0: ClassTag[U]): U**

    Aggregate the elements of each partition, and then the results for all the partitions, using given combine functions and a neutral "zero value". This function can return a different result type, U, than the type of this RDD, T. Thus, we need one operation for merging a T into an U and one operation for merging two U's, as in scala.TraversableOnce. Both of these functions are allowed to modify and return their first argument instead of creating a new U to avoid memory allocation.
    
    **zeroValue**
the initial value for the accumulated result of each partition for the seqOp operator, and also the initial value for the combine results from different partitions for the combOp operator - this will typically be the neutral element (e.g. Nil for list concatenation or 0 for summation)

    **seqOp**（聚合函数）
an operator used to accumulate results within a partition

    **combOp**（统计函数）
an associative operator used to combine results from different partitions

{%highlight scala%}
scala> val z = sc.parallelize(Array("a","b"),2)
z: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[37] at parallelize at <console>:27

/*函数执行过程
seq:x+a
com:x+xa
seq:x+b
com:xxa+xb
res69: String = xxaxb
*/

scala> z.aggregate("x")(_ + _, _+_)
res69: String = xxbxa

{%endhighlight%}


* def
**cache(): RDD.this.type**

    Persist this RDD with the default storage level (MEMORY_ONLY).

* def
**cartesian[U] (other: RDD[U])(implicit arg0: ClassTag[U]): RDD[(T, U)]**(两个rdd做笛卡尔积)
    Return the Cartesian product of this RDD and another one, that is, the RDD of all pairs of elements (a, b) where a is in this and b is in other.

{%highlight scala%}
//rdd1=1,2,3;rdd3=a,b
scala> val rdd4 = rdd3.cartesian(rdd1)
rdd4: org.apache.spark.rdd.RDD[(String, Int)] = CartesianRDD[6] at cartesian at <console>:33

scala> rdd4.collect
res7: Array[(String, Int)] = Array((a,1), (a,3), (a,2), (b,1), (b,3), (b,2))

{%endhighlight%}


* def
**collect[U] (f: PartialFunction[T, U](Scala偏函数))(implicit arg0: ClassTag[U]): RDD[U]**
Return an RDD that contains all matching values by applying f.

{%highlight scala%}
//定义偏函数,大于1的元素+1
scala> val pfunc : PartialFunction[Int,Int] = { case s if s>1 => s+1 }
pfunc: PartialFunction[Int,Int] = <function1>

scala> rdd1.collect
res29: Array[Int] = Array(1, 3, 2)

scala> val rdd5 = rdd1.collect(pfunc)
rdd5: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[14] at collect at <console>:33

scala> rdd5.collect
res30: Array[Int] = Array(4, 3)


{%endhighlight%}

* def
**distinct(): RDD[T]**

    Return a new RDD containing the distinct elements in this RDD.（去重函数）

* def
**distinct(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T]**

    Return a new RDD containing the distinct elements in this RDD.

* def
**count(): Long**

    Return the number of elements in the RDD.(计数函数)

* def
**countApprox(timeout: Long, confidence: Double = 0.95): PartialResult[BoundedDouble]**

    Approximate version of count() that returns a potentially incomplete result within a timeout, even if not all tasks have finished.（计算时间和计算可信度）


* def
**countApproxDistinct(relativeSD: Double = 0.05): Long**

    Return approximate number of distinct elements in the RDD.（统计不同的元素）

    The algorithm used is based on streamlib's implementation of "HyperLogLog in Practice: Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm", available here.
relativeSD
Relative accuracy. Smaller values create counters that require more space. It must be greater than 0.000017.

{% highlight scala%}

scala> val data = Array(1,1,2,3,21)
data: Array[Int] = Array(1, 1, 2, 3, 21)

scala> distrdd.count
res1: Long = 5                                                                  
                      ^
//100毫秒计算时间
scala> distrdd.countApprox(100)
res3: org.apache.spark.partial.PartialResult[org.apache.spark.partial.BoundedDouble] = (final: [5.000, 5.000])

//不同元素有四个
scala> distrdd.countApproxDistinct(0.05)
res4: Long = 4


{% endhighlight%}



* def
**filter(f: (T) ⇒ Boolean): RDD[T]**

    Return a new RDD containing only the elements that satisfy a predicate.(筛选函数)

{% highlight scala%}
scala> val distrdd2 = distrdd.filter(s => s/2==0)
distrdd2: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[5] at filter at <console>:31

{% endhighlight%}


PairRDDFunctions
----------------

Extra functions available on RDDs of (key, value) pairs through an implicit conversion.
在键值对中的RDD方法

* def
**reduceByKey(func: (V, V) ⇒ V): RDD[(K, V)]**
    
    Merge the values for each key using an associative and commutative reduce function. This will also perform the merging locally on each mapper before sending results to a reducer, similarly to a "combiner" in MapReduce. Output will be hash-partitioned with the existing partitioner/ parallelism level.


* def
**join[W](other: RDD[(K, W)]): RDD[(K, (V, W))]**
    
    Return an RDD containing all pairs of elements with matching keys in this and other. Each pair of elements will be returned as a (k, (v1, v2)) tuple, where (k, v1) is in this and (k, v2) is in other. Performs a hash join across the cluster.


{% highlight scala%}
  //PairRDDFunctions
    val rdd4 = sc.parallelize(Array(("hello",1),("pi",1)))
    val data1 = "hello world hello pi pi haha"
    //统计每个单词个数
    val rdd3 = sc.parallelize(data1.split(" "))
      .map(i => (i,1)).reduceByKey(_+_)
    //合并相同单词
    val rdd5 = rdd3.join[Int](rdd4)

    rdd3.foreach(i => println(i))
    rdd5.foreach(i => println(i))

    /**
      * 运行结果1
      * (haha,1)
      * (hello,2)
      * (world,1)
      * (pi,2)
      * 
      * 运行结果2
      * (pi,(2,1))
      * (hello,(2,1))
      */

{% endhighlight%}

常用 Api
---------------

{%highlight scala%}

//Transformations
      println("============="+"Transformations"+"============")

    val numData = Array(1,2,2,3)
    val stringArray = Array("pi","huaiyu","hello","pi")
    val stringData = "pi huai yu hello pi"
    val stringArrayData = Array("hello world pi","pi huai yu")

    val numRdd = sc.parallelize(numData)
    val stringRdd = sc.parallelize(stringData)
    /**
      * map(func)
      * Return a new distributed dataset formed by
      * passing each element of the source through a function func.
      */
    println("---------"+"map"+"----------")
    val mapRdd = numRdd.map(_+1)
    mapRdd.foreach(println(_))

    /**
      * filter(func)
      * Return a new dataset formed by selecting those
      * elements of the source on which func returns true.
      */
    println("---------"+"filter"+"----------")
    val filterRdd = numRdd.filter(i => i<2)
    filterRdd.foreach(println(_))

    /**
      * flatMap(func)
      * Similar to map, but each input item can be
      * mapped to 0 or more output items (so func should return a Seq rather than a single item).
      * flatmap和map的不同在于，flatmap从m映射出n个平行元素，map从n映射出相等n个元素
      */
    println("---------"+"flatMap"+"----------")
    val flatMapRdd1 = sc.parallelize(stringArrayData).flatMap(_.split(" "))
    flatMapRdd1.foreach(println(_))
    println("---------"+"map"+"----------")
    val flatMapRdd2 = sc.parallelize(stringArrayData).map(_.split(" "))
    flatMapRdd2.foreach(i => {
      println("item")
      i.foreach(println(_))
    }
    )

    /**
      * mapPartitions(func)
      * 该函数和map函数类似，只不过映射函数的参数由RDD中的每一个元素变成了RDD中每一个分区的迭代器。
      * 如果在映射的过程中需要频繁创建额外的对象，使用mapPartitions要比map高效的过。
      * 参数preservesPartitioning表示是否保留父RDD的partitioner分区信息。
      */
    println("---------"+"mapPartitions"+"----------")
    val mapPartitionsRdd = sc.parallelize(stringArrayData).mapPartitions(_.map(_.split(" ")))
    mapPartitionsRdd.foreach(_.foreach(println(_)))

    /**
      * mapPartitionsWithIndex(func)
      * Similar to mapPartitions, but also provides func with an integer value representing the index of the          partition,
      * so func must be of type (Int, Iterator<T>) => Iterator<U> when running on an RDD of type T.
      */
    println("---------"+"mapPartitionsWithIndex"+"----------")
    val mapPartitionsWithIndexRdd = sc.parallelize(stringArrayData,2).mapPartitionsWithIndex((a,b) =>{
      if(a == 1) {
        println(a)
        b.map(_.split(""))
      }
      else {
        println(a)
        b.map(_.split(" "))
      }
    }
    )
    mapPartitionsWithIndexRdd.foreach(_.foreach(println(_)))

    /**
      * sample(withReplacement, fraction, seed)
      * Sample a fraction fraction of the data, with or without replacement,
      * using a given random number generator seed.
      * 对RDD中的集合内元素进行采样，第一个参数withReplacement是true表示有放回取样，false表示无放回。
      * 第二个参数表示比例，第三个参数是随机种子。
      */
    println("---------"+"sample"+"----------")
    val sampleRdd = numRdd.sample(false,0.5)
    sampleRdd.foreach(println(_))

    /**
      *   union(otherDataset)
      * 	Return a new dataset that contains the union of
      * 	the elements in the source dataset and the argument.
      */

    println("---------"+"union"+"----------")
    val unionRdd = numRdd.union(sampleRdd)
    unionRdd.foreach(println(_))

    /**
      * intersection(otherDataset)
      * Return a new RDD that contains the intersection of
      * elements in the source dataset and the argument.
      */
    println("---------"+"intersection"+"----------")
    val intersectionRdd = numRdd.intersection(sampleRdd)
    intersectionRdd.foreach(println(_))

    /**
      * distinct([numTasks]))
      * Return a new dataset that contains the distinct elements of the source dataset.
      */
    println("---------"+"distinct"+"----------")
    val distinctRdd = numRdd.distinct()
    distinctRdd.foreach(println(_))

    /**
      *   groupByKey([numTasks])
      * 	When called on a dataset of (K, V) pairs, returns a dataset of (K, Iterable<V>) pairs.
      * 	Note: If you are grouping in order to perform an aggregation (such as a sum or average) over each key,
      * 	using reduceByKey or aggregateByKey will yield much better performance.
      * 	Note: By default, the level of parallelism in the output depends on the number of
      * 	partitions of the parent RDD. You can pass an optional numTasks argument to set a different number of tasks.
      */
    println("---------"+"groupByKey"+"----------")
    val groupByKeyRdd = sc.parallelize(stringArrayData).flatMap(_.split(" ")).map((_,1)).groupByKey()
    groupByKeyRdd.foreach(println(_))

    /**
      *   reduceByKey(func, [numTasks])
      * 	When called on a dataset of (K, V) pairs, returns a dataset of (K, V) pairs
      * 	where the values for each key are aggregated using the given reduce function func,
      * 	which must be of type (V,V) => V. Like in groupByKey,
      * 	the number of reduce tasks is configurable through an optional second argument.
      */
    println("---------"+"reduceByKey"+"----------")
    val reduceByKeyRdd = sc.parallelize(stringArrayData).flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_)
    reduceByKeyRdd.foreach(println(_))

    /**
      * aggregateByKey(zeroValue)(seqOp, combOp, [numTasks])
      * Aggregate the values of each key, using given combine functions and a neutral "zero value".
      * This function can return a different result type, U, than the type of the values in this RDD, V.
      * Thus, we need one operation for merging a V into a U and one operation for merging two U's, as in scala.
      * TraversableOnce. The former operation is used for merging values within a partition,
      * and the latter is used for merging values between partitions. To avoid memory allocation,
      * both of these functions are allowed to modify and return their first argument instead of creating a new U.
      * 聚合键值对的value，通过seqOp方法可以聚合不同类型的value。例如(K,V)，zerovalue类型为U，通过seqOp方法返回U类型的值。
      * 最终通过comOp方法将所有value聚合。
      */
    println("---------"+"aggregateByKey"+"----------")
    val aggregateByKeyRdd = sc.parallelize(stringArrayData).flatMap(_.split(" "))
      .map((_,"a")).aggregateByKey(1)((a,b) => a,_+_)
    aggregateByKeyRdd.foreach(println(_))

    /**
      * sortByKey([ascending], [numTasks])
      * When called on a dataset of (K, V) pairs where K implements Ordered,
      * returns a dataset of (K, V) pairs sorted by keys in ascending or descending order,
      * as specified in the boolean ascending argument.
      */
    println("---------"+"sortByKey"+"----------")
    val sortByKeyRdd = sc.parallelize(stringArrayData,1).flatMap(_.split(" "))
      .map((_,"a")).sortByKey()
    sortByKeyRdd.foreach(println(_))

    /**
      * join(otherDataset, [numTasks])
      * When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (V, W)) pairs
      * with all pairs of elements for each key. Outer joins are supported through leftOuterJoin,
      * rightOuterJoin, and fullOuterJoin.
      */
    println("---------"+"join"+"----------")
    val stringDataTestJoin = "pi huai yu"
    val joinRdd1 = sc.parallelize(stringArrayData).flatMap(_.split(" "))
      .map((_,"a"))
    val joinRdd2 = sc.parallelize(stringDataTestJoin.split(" ")).map((_,1))
    val joinRdd3 = joinRdd1.join(joinRdd2)
    joinRdd3.foreach(println(_))

    /**
      *   cogroup(otherDataset, [numTasks])
      * 	When called on datasets of type (K, V) and (K, W), returns a dataset of
      * 	(K, (Iterable<V>, Iterable<W>)) tuples. This operation is also called groupWith.
      *   相比于join，cogroup会将Key相同的元素全部聚合
      */

    println("---------"+"cogroup"+"----------")
    val cogroupRdd = joinRdd1.cogroup(joinRdd2)
    cogroupRdd.foreach(println(_))

    /**
      * cartesian(otherDataset)
      * When called on datasets of types T and U, returns a dataset of (T, U) pairs (all pairs of elements).
      */
    println("---------"+"cartesian"+"----------")
    val cartesianRdd = stringRdd.cartesian(numRdd)
    cartesianRdd.foreach(println(_))

    /**
      * pipe(command, [envVars])
      * Pipe each partition of the RDD through a shell command,
      * e.g. a Perl or bash script. RDD elements are written to the process's stdin and lines output to
      * its stdout are returned as an RDD of strings.
      * 如何使用？
      */

    /**
      * coalesce(numPartitions)
      * Decrease the number of partitions in the RDD to numPartitions.
      * Useful for running operations more efficiently after filtering down a large dataset.
      */
    println("---------"+"coalesce"+"----------")
    val coalesceRdd = numRdd.filter(i => i<2).coalesce(1)
    coalesceRdd.foreach(println(_))

    /**
      *   repartition(numPartitions)
      * 	Reshuffle the data in the RDD randomly to create either more or fewer partitions and balance it across them.
      * 	This always shuffles all data over the network.
      */
    println("---------"+"repartition"+"----------")
    val repatitionRdd = numRdd.repartition(2)
    repatitionRdd.foreach(println(_))

    /**
      * repartitionAndSortWithinPartitions(partitioner)
      * Repartition the RDD according to the given partitioner and, within each resulting partition,
      * sort records by their keys. This is more efficient than calling repartition and then
      * sorting within each partition because it can push the sorting down into the shuffle machinery.
      * api中没看见？
      */

    //Actions

    println("============="+"Actions"+"============")
    println("---------"+"reduce"+"----------")
    val reduceArray = numRdd.reduce(_+_)
    println(reduceArray)

    println("---------"+"count"+"----------")
    val count = numRdd.count()
    println(count)

    println("---------"+"first"+"----------")
    println(numRdd.first())

    println("---------"+"take"+"----------")
    numRdd.take(2).foreach(println(_))

    /**
      * takeSample(withReplacement, num, [seed])
      * Return an array with a random sample of num elements of the dataset,
      * with or without replacement, optionally pre-specifying a random number generator seed.
      */

    println("---------"+"takeSample"+"----------")
    numRdd.takeSample(false,2).foreach(println(_))

    /**
      * takeOrdered(n, [ordering])
      * Return the first n elements of the RDD using either their natural order or a custom comparator.
      */
    println("---------"+"takeOrdered"+"----------")
    numRdd.takeOrdered(2).foreach(println(_))

//    println("---------"+"saveAsTextFile"+"----------")
//    numRdd.saveAsTextFile("/home/pi/doc/sparkout/TextFile")
//
//    println("---------"+"saveAsObjectFile"+"----------")
//    numRdd.saveAsObjectFile("/home/pi/doc/sparkout/ObjectFile")
//
//    println("---------"+"saveAsSequenceFile"+"----------")
//    sc.parallelize(stringData.split(" ")).map((_,1)).saveAsSequenceFile("/home/pi/doc/sparkout/SequenceFile")

    println("---------"+"countByKey"+"----------")
    sc.parallelize(stringData.split(" ")).map((_,1)).countByKey().foreach(println(_))
    
{%endhighlight%}