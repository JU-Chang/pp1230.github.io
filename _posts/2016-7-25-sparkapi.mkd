---
layout: post
title: Spark Api
description: Spark Api 学习
keywords: Spark,rdd
---

SPARK API
=============

> Spark Api Docs : <http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.package>

SparkContext
-------------

* def
**parallelize[T] (seq: Seq[T], numSlices: Int = defaultParallelism(分组参数，可以为空))(implicit arg0: ClassTag[T]): RDD[T]**

    Distribute a local Scala collection to form an RDD.

    **Note**:
avoid using parallelize(Seq()) to create an empty RDD. Consider emptyRDD for an RDD with no partitions, or parallelize(Seq[T]()) for an RDD of T with empty partitions.
,
Parallelize acts lazily. If seq is a mutable collection and is altered after the call to parallelize and before the first action on the RDD, the resultant RDD will reflect the modified collection. Pass a copy of the argument to avoid this.

* def
**textFile(path: String, minPartitions: Int = defaultMinPartitions): RDD[String]**
    Read a text file from HDFS, a local file system (available on all nodes), or any Hadoop-supported file system URI, and return it as an RDD of Strings.

{%highlight scala%}
//从本地文件初始化RDD
scala> val rdd2 = sc.textFile("data.txt")
rdd2: org.apache.spark.rdd.RDD[String] = data.txt MapPartitionsRDD[3] at textFile at <console>:27

scala> rdd2.toArray
res2: Array[String] = Array(1, 11, 222, 3333, 44444)

{%endhighlight%}


RDD
----------


A Resilient Distributed Dataset (RDD), the basic abstraction in Spark. Represents an immutable, partitioned collection of elements that can be operated on in parallel. This class contains the basic operations available on all RDDs, such as map, filter, and persist. In addition, org.apache.spark.rdd.**PairRDDFunctions** contains operations available only on RDDs of key-value pairs, such as groupByKey and join; org.apache.spark.rdd.**DoubleRDDFunctions** contains operations available only on RDDs of Doubles; and org.apache.spark.rdd.**SequenceFileRDDFunctions** contains operations available on RDDs that can be saved as SequenceFiles. All operations are automatically available on any RDD of the right type (e.g. RDD[(Int, Int)] through implicit.

RDD是Spark的基本抽象类，包含了可以分布式运算的集合元素。RDD类包含了基本的map filtet persist方法，当RDD是键值对和Double时有额外的方法。


* def
**++(other: RDD[T]): RDD[T]**

    Return the union of this RDD and another one(合并两个RDD). Any identical elements will appear multiple times (use .distinct() to eliminate them).

* 
def
**aggregate[U] (zeroValue : U)(seqOp: (U, T) ⇒ U, combOp: (U, U) ⇒ U)(implicit arg0: ClassTag[U]): U**

    Aggregate the elements of each partition, and then the results for all the partitions, using given combine functions and a neutral "zero value". This function can return a different result type, U, than the type of this RDD, T. Thus, we need one operation for merging a T into an U and one operation for merging two U's, as in scala.TraversableOnce. Both of these functions are allowed to modify and return their first argument instead of creating a new U to avoid memory allocation.
    
    **zeroValue**
the initial value for the accumulated result of each partition for the seqOp operator, and also the initial value for the combine results from different partitions for the combOp operator - this will typically be the neutral element (e.g. Nil for list concatenation or 0 for summation)

    **seqOp**（聚合函数）
an operator used to accumulate results within a partition

    **combOp**（统计函数）
an associative operator used to combine results from different partitions

{%highlight scala%}
scala> val z = sc.parallelize(Array("a","b"),2)
z: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[37] at parallelize at <console>:27

/*函数执行过程
seq:x+a
com:x+xa
seq:x+b
com:xxa+xb
res69: String = xxaxb
*/

scala> z.aggregate("x")(_ + _, _+_)
res69: String = xxbxa

{%endhighlight%}


* def
**cache(): RDD.this.type**

    Persist this RDD with the default storage level (MEMORY_ONLY).

* def
**cartesian[U] (other: RDD[U])(implicit arg0: ClassTag[U]): RDD[(T, U)]**(两个rdd做笛卡尔积)
    Return the Cartesian product of this RDD and another one, that is, the RDD of all pairs of elements (a, b) where a is in this and b is in other.

{%highlight scala%}
//rdd1=1,2,3;rdd3=a,b
scala> val rdd4 = rdd3.cartesian(rdd1)
rdd4: org.apache.spark.rdd.RDD[(String, Int)] = CartesianRDD[6] at cartesian at <console>:33

scala> rdd4.collect
res7: Array[(String, Int)] = Array((a,1), (a,3), (a,2), (b,1), (b,3), (b,2))

{%endhighlight%}


* def
**collect[U] (f: PartialFunction[T, U](Scala偏函数))(implicit arg0: ClassTag[U]): RDD[U]**
Return an RDD that contains all matching values by applying f.

{%highlight scala%}
//定义偏函数,大于1的元素+1
scala> val pfunc : PartialFunction[Int,Int] = { case s if s>1 => s+1 }
pfunc: PartialFunction[Int,Int] = <function1>

scala> rdd1.collect
res29: Array[Int] = Array(1, 3, 2)

scala> val rdd5 = rdd1.collect(pfunc)
rdd5: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[14] at collect at <console>:33

scala> rdd5.collect
res30: Array[Int] = Array(4, 3)


{%endhighlight%}

* def
**distinct(): RDD[T]**

    Return a new RDD containing the distinct elements in this RDD.（去重函数）

* def
**distinct(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T]**

    Return a new RDD containing the distinct elements in this RDD.

* def
**count(): Long**

    Return the number of elements in the RDD.(计数函数)

* def
**countApprox(timeout: Long, confidence: Double = 0.95): PartialResult[BoundedDouble]**

    Approximate version of count() that returns a potentially incomplete result within a timeout, even if not all tasks have finished.（计算时间和计算可信度）


* def
**countApproxDistinct(relativeSD: Double = 0.05): Long**

    Return approximate number of distinct elements in the RDD.（统计不同的元素）

    The algorithm used is based on streamlib's implementation of "HyperLogLog in Practice: Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm", available here.
relativeSD
Relative accuracy. Smaller values create counters that require more space. It must be greater than 0.000017.

{% highlight scala%}

scala> val data = Array(1,1,2,3,21)
data: Array[Int] = Array(1, 1, 2, 3, 21)

scala> distrdd.count
res1: Long = 5                                                                  
                      ^
//100毫秒计算时间
scala> distrdd.countApprox(100)
res3: org.apache.spark.partial.PartialResult[org.apache.spark.partial.BoundedDouble] = (final: [5.000, 5.000])

//不同元素有四个
scala> distrdd.countApproxDistinct(0.05)
res4: Long = 4


{% endhighlight%}



* def
**filter(f: (T) ⇒ Boolean): RDD[T]**

    Return a new RDD containing only the elements that satisfy a predicate.(筛选函数)

{% highlight scala%}
scala> val distrdd2 = distrdd.filter(s => s/2==0)
distrdd2: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[5] at filter at <console>:31

{% endhighlight%}


PairRDDFunctions
----------------

Extra functions available on RDDs of (key, value) pairs through an implicit conversion.
在键值对中的RDD方法

* def
**reduceByKey(func: (V, V) ⇒ V): RDD[(K, V)]**
    
    Merge the values for each key using an associative and commutative reduce function. This will also perform the merging locally on each mapper before sending results to a reducer, similarly to a "combiner" in MapReduce. Output will be hash-partitioned with the existing partitioner/ parallelism level.


* def
**join[W](other: RDD[(K, W)]): RDD[(K, (V, W))]**
    
    Return an RDD containing all pairs of elements with matching keys in this and other. Each pair of elements will be returned as a (k, (v1, v2)) tuple, where (k, v1) is in this and (k, v2) is in other. Performs a hash join across the cluster.


{% highlight scala%}
  //PairRDDFunctions
    val rdd4 = sc.parallelize(Array(("hello",1),("pi",1)))
    val data1 = "hello world hello pi pi haha"
    //统计每个单词个数
    val rdd3 = sc.parallelize(data1.split(" "))
      .map(i => (i,1)).reduceByKey(_+_)
    //合并相同单词
    val rdd5 = rdd3.join[Int](rdd4)

    rdd3.foreach(i => println(i))
    rdd5.foreach(i => println(i))

    /**
      * 运行结果1
      * (haha,1)
      * (hello,2)
      * (world,1)
      * (pi,2)
      * 
      * 运行结果2
      * (pi,(2,1))
      * (hello,(2,1))
      */

{% endhighlight%}
