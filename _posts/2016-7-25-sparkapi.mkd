---
layout: post
title: Spark Api
description: Spark Api 学习
keywords: Spark,rdd
---

SPARK API
=============

> Spark Api Docs : http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.package

SparkContext
-------------

* def
**parallelize[T] (seq: Seq[T], numSlices: Int = defaultParallelism(分组参数，可以为空))(implicit arg0: ClassTag[T]): RDD[T]**

    Distribute a local Scala collection to form an RDD.

    **Note**:
avoid using parallelize(Seq()) to create an empty RDD. Consider emptyRDD for an RDD with no partitions, or parallelize(Seq[T]()) for an RDD of T with empty partitions.
,
Parallelize acts lazily. If seq is a mutable collection and is altered after the call to parallelize and before the first action on the RDD, the resultant RDD will reflect the modified collection. Pass a copy of the argument to avoid this.

* def
**textFile(path: String, minPartitions: Int = defaultMinPartitions): RDD[String]**
    Read a text file from HDFS, a local file system (available on all nodes), or any Hadoop-supported file system URI, and return it as an RDD of Strings.

{%highlight scala%}
//从本地文件初始化RDD
scala> val rdd2 = sc.textFile("data.txt")
rdd2: org.apache.spark.rdd.RDD[String] = data.txt MapPartitionsRDD[3] at textFile at <console>:27

scala> rdd2.toArray
res2: Array[String] = Array(1, 11, 222, 3333, 44444)

{%endhighlight%}


RDD
----------

* def
**++(other: RDD[T]): RDD[T]**

    Return the union of this RDD and another one(合并两个RDD). Any identical elements will appear multiple times (use .distinct() to eliminate them).

* 
def
**aggregate[U] (zeroValue : U)(seqOp: (U, T) ⇒ U, combOp: (U, U) ⇒ U)(implicit arg0: ClassTag[U]): U**

    Aggregate the elements of each partition, and then the results for all the partitions, using given combine functions and a neutral "zero value". This function can return a different result type, U, than the type of this RDD, T. Thus, we need one operation for merging a T into an U and one operation for merging two U's, as in scala.TraversableOnce. Both of these functions are allowed to modify and return their first argument instead of creating a new U to avoid memory allocation.
    
    **zeroValue**
the initial value for the accumulated result of each partition for the seqOp operator, and also the initial value for the combine results from different partitions for the combOp operator - this will typically be the neutral element (e.g. Nil for list concatenation or 0 for summation)

    **seqOp**（聚合函数）
an operator used to accumulate results within a partition

    **combOp**（统计函数）
an associative operator used to combine results from different partitions

{%highlight scala%}
scala> val z = sc.parallelize(Array("a","b"),2)
z: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[37] at parallelize at <console>:27

/*函数执行过程
seq:x+a
com:x+xa
seq:x+b
com:xxa+xb
res69: String = xxaxb
*/

scala> z.aggregate("x")(_ + _, _+_)
res69: String = xxbxa

{%endhighlight%}


* def
**cache(): RDD.this.type**

    Persist this RDD with the default storage level (MEMORY_ONLY).

* def
**cartesian[U](other: RDD[U])(implicit arg0: ClassTag[U]): RDD[(T, U)]**(两个rdd做笛卡尔积)
    Return the Cartesian product of this RDD and another one, that is, the RDD of all pairs of elements (a, b) where a is in this and b is in other.

{%highlight scala%}
//rdd1=1,2,3;rdd3=a,b
scala> val rdd4 = rdd3.cartesian(rdd1)
rdd4: org.apache.spark.rdd.RDD[(String, Int)] = CartesianRDD[6] at cartesian at <console>:33

scala> rdd4.collect
res7: Array[(String, Int)] = Array((a,1), (a,3), (a,2), (b,1), (b,3), (b,2))

{%endhighlight%}


* def
**collect[U] (f: PartialFunction[T, U](Scala偏函数))(implicit arg0: ClassTag[U]): RDD[U]**
Return an RDD that contains all matching values by applying f.

{%highlight scala%}
//定义偏函数,大于1的元素+1
scala> val pfunc : PartialFunction[Int,Int] = { case s if s>1 => s+1 }
pfunc: PartialFunction[Int,Int] = <function1>

scala> rdd1.collect
res29: Array[Int] = Array(1, 3, 2)

scala> val rdd5 = rdd1.collect(pfunc)
rdd5: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[14] at collect at <console>:33

scala> rdd5.collect
res30: Array[Int] = Array(4, 3)


{%endhighlight%}

* def
**distinct(): RDD[T]**

    Return a new RDD containing the distinct elements in this RDD.（去重函数）

* def
**distinct(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T]**

    Return a new RDD containing the distinct elements in this RDD.

* def
**count(): Long**

    Return the number of elements in the RDD.(计数函数)

* def
**countApprox(timeout: Long, confidence: Double = 0.95): PartialResult[BoundedDouble]**

    Approximate version of count() that returns a potentially incomplete result within a timeout, even if not all tasks have finished.（计算时间和计算可信度）


* def
**countApproxDistinct(relativeSD: Double = 0.05): Long**

    Return approximate number of distinct elements in the RDD.（统计不同的元素）

    The algorithm used is based on streamlib's implementation of "HyperLogLog in Practice: Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm", available here.
relativeSD
Relative accuracy. Smaller values create counters that require more space. It must be greater than 0.000017.

{% highlight scala%}

scala> val data = Array(1,1,2,3,21)
data: Array[Int] = Array(1, 1, 2, 3, 21)

scala> distrdd.count
res1: Long = 5                                                                  
                      ^
//100毫秒计算时间
scala> distrdd.countApprox(100)
res3: org.apache.spark.partial.PartialResult[org.apache.spark.partial.BoundedDouble] = (final: [5.000, 5.000])

//不同元素有四个
scala> distrdd.countApproxDistinct(0.05)
res4: Long = 4


{% endhighlight%}



* def
**filter(f: (T) ⇒ Boolean): RDD[T]**

    Return a new RDD containing only the elements that satisfy a predicate.(筛选函数)

{% highlight scala%}
scala> val distrdd2 = distrdd.filter(s => s/2==0)
distrdd2: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[5] at filter at <console>:31

{% endhighlight%}

