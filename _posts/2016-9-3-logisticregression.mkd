---
layout: post
title: 机器学习算法之逻辑回归
description: 逻辑回归(Logistic Regression, LR)模型其实仅在线性回归的基础上，套用了一个逻辑函数，但也就由于这个逻辑函数，使得逻辑回归模型成为了机器学习领域一颗耀眼的明星，更是计算广告学的核心。
keywords: logistic regression,spark
---

> 逻辑回归就是一种减小预测范围，将预测值限定为[0,1]间的一种回归模型，其回归方程与回归曲线如图2所示。逻辑曲线在z=0时，十分敏感，在z>>0或z<<0处，都不敏感，将预测值限定为(0,1)。



1.逻辑回归算法原理
------------
<p>
    &nbsp;&nbsp;&nbsp; 逻辑回归其实仅为在线性回归的基础上，套用了一个逻辑函数，但也就由于这个逻辑函数，逻辑回归成为了机器学习领域一颗耀眼的明星，更是计算广告学的核心。对于多元逻辑回归，可用如下公式似合分类，其中公式(4)的变换，将在逻辑回归模型参数估计时，化简公式带来很多益处，y={0,1}为分类结果。    <a href="file:///C:\Users\wensaiping\AppData\Local\Temp\WindowsLiveWriter1286139640\supfilesCE8402E\image811.png" rel="nofollow">
      <br/>
    </a>

  </p>
  
  ![pic](../../../static/images/lor1.gif)
  
  <p>
    &nbsp;&nbsp;&nbsp; 对于训练数据集，特征数据x={x    <sub>1</sub>
    , x    <sub>2</sub>
    , … , x    <sub>m</sub>
    }和对应的分类数据y={y    <sub>1</sub>
    , y    <sub>2</sub>
    , … , y    <sub>m</sub>
    }。构建逻辑回归模型f(θ)，最典型的构建方法便是应用极大似然估计。首先，对于单个样本，其后验概率为：  </p>
    
![pic](../../../static/images/lor2.gif)
  <p>

        &nbsp;&nbsp;&nbsp; 那么，极大似然函数为：  </p>
  ![pic](../../../static/images/lor3.gif)
  <p>

        &nbsp;&nbsp;&nbsp; log似然是：  </p>
![pic](../../../static/images/lor4.gif)
  <p>

      </p>
  <h4>
    <b>2</b>
    <b>梯度下降</b>
    <b></b>
  </h4>
  <p>&nbsp;&nbsp;&nbsp; 由第1节可知，求逻辑回归模型f(θ)，等价于：</p>
![pic](../../../static/images/lor5.gif)
  <p>
        &nbsp;&nbsp;&nbsp; 采用梯度下降法：  </p>
![pic](../../../static/images/lor6.gif)
  <p>

        &nbsp;&nbsp;&nbsp;&nbsp; 从而迭代θ至收敛即可：  </p>
![pic](../../../static/images/lor7.gif)

2.sparkML测试

{%highlight scala%}
   val ss = SparkSession.builder().appName("Machine Learning Test")
      .master("local[*]").getOrCreate()

    println("----------Logistic Regression Example------------")
    val training = ss.createDataFrame(Seq(
      (1.0, Vectors.dense(0.0, 1.1, 0.1)),
      (0.0, Vectors.dense(2.0, 1.0, -1.0)),
      (0.0, Vectors.dense(2.0, 1.3, 1.0)),
      (1.0, Vectors.dense(0.0, 1.2, -0.5))
    )).toDF("label", "features")
    val lr = new LogisticRegression()
    println("LogisticRegression parameters:\n" + lr.explainParams() + "\n")
    lr.setMaxIter(10)
      .setRegParam(0.01)
    val logicalModel = lr.fit(training)
    println("logicalModel was fit using parameters: " + logicalModel.parent.extractParamMap)

    // We may alternatively specify parameters using a ParamMap,
    // which supports several methods for specifying parameters.
    val paramMap = ParamMap(lr.maxIter -> 20)
      .put(lr.maxIter, 30)  // Specify 1 Param. This overwrites the original maxIter.
      .put(lr.regParam -> 0.1, lr.threshold -> 0.55)  // Specify multiple Params.

    // One can also combine ParamMaps.
    val paramMap2 = ParamMap(lr.probabilityCol -> "myProbability")  // Change output column name.
    val paramMapCombined = paramMap ++ paramMap2

    // Now learn a new model using the paramMapCombined parameters.
    // paramMapCombined overrides all parameters set earlier via lr.set* methods.
    val model2 = lr.fit(training, paramMapCombined)
    println("Model 2 was fit using parameters: " + model2.parent.extractParamMap)

    // Prepare test data.
    val test = ss.createDataFrame(Seq(
      (1.0, Vectors.dense(-1.0, 1.5, 1.3)),
      (0.0, Vectors.dense(3.0, 2.0, -0.1)),
      (1.0, Vectors.dense(0.0, 2.2, -1.5))
    )).toDF("label", "features")

    // Make predictions on test data using the Transformer.transform() method.
    // LogisticRegression.transform will only use the 'features' column.
    // Note that model2.transform() outputs a 'myProbability' column instead of the usual
    // 'probability' column since we renamed the lr.probabilityCol parameter previously.
    model2.transform(test)
      .select("features", "label", "myProbability", "prediction")
      .collect()
      .foreach { case Row(features: org.apache.spark.ml.linalg.Vector ,
      label: Double, prob: org.apache.spark.ml.linalg.Vector , prediction: Double) =>
        println(s"($features, $label) -> prob=$prob, prediction=$prediction")
      }
{%endhighlight%}
