<!DOCTYPE html>
<html id="J-html" class="">
<head>
    <meta charset="UTF-8" />
    <title>
        
            机器学习算法之线性回归
        
    </title>
    <meta name="generator" content="Jekyll" />
    <meta name="author" content="phy" />
    <meta name="description" content="线性回归，是利用数理统计中回归分析，来确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法" />
    <meta name="keywords" content="linear regression,spark" />
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
   <link rel="stylesheet" type="text/css" media="all" href="../../../static/style.css" />
    <link rel="stylesheet" type="text/css" media="all" href="../../../static/pygments.css" />

    <!--[if lt IE 9]>
    <script src="http://pp1230.github.io/static/js/html5.js" type="text/javascript"></script>
    <![endif]-->
    <script src="http://pp1230.github.io/static/js/jquery.js" type="text/javascript"></script>
<script type="text/javascript"
 src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</head>
<body itemscope itemtype="http://schema.org/WebPage" class="home blog lotus index">
    <nav class="lotus-nav">
        <ul>
            
            
            
            
            
                
            
            <li class="home ">
                <a href="/index.html" rel="bookmark" title="首页">
                    <i class="icon-home"></i>
                </a>
                
            </li>
            
            
            
            
            
            <li class="">
                <a href="/archives.html" rel="bookmark" title="文章归档">
                    <i class="icon-reorder"></i>
                </a>
                
            </li>
            
            
            
            
            
            <li class="">
                <a href="/contact.html" rel="bookmark" title="关于我">
                    <i class="icon-envelope-alt"></i>
                </a>
                
            </li>
            
            
            
            
            
            <li class="">
                <a href="/love/pwd.html" rel="bookmark" title="love">
                    <i class="icon-heart"></i>
                </a>
                
            </li>
            
            
            
            
            
            <li class="">
                <a href="/resource.html" rel="bookmark" title="资源">
                    <i class="icon-github"></i>
                </a>
                
            </li>
            
        </ul>
    </nav>

    <p class="lotus-breadcrub">
    <a href="http://pp1230.github.io/index.html" rel="nofollow" rel="nofollow" title="首页">Home</a>
    <span> &gt; </span>
    <a href="http://pp1230.github.io/archives.html" rel="nofollow" >Archives</a>
    <span> &gt; </span>
    机器学习算法之线性回归
</p>
<h1 class="lotus-pagetit">机器学习算法之线性回归</h1>
<p class="lotus-meta">Publish: <time class="date" pubdate="August 25, 2016">August 25, 2016</time></p>
<article  itemscope itemtype="http://schema.org/Article" class="lotus-post">
<h1 id="section">线性回归</h1>

<p>在统计学中，线性回归(Linear Regression)是利用称为线性回归方程的最小平方函数对一个或多个自变量和因变量之间关系进行建模的一种回归分析。这种函数是一个或多个称为回归系数的模型参数的线性组合。只有一个自变量的情况称为简单回归,大于一个自变量情况的叫做多元回归。</p>

<h2 id="section-1">线性回归求解方法</h2>

<p><strong>1.梯度下降法</strong></p>

<p>下面是一个典型的机器学习的过程，首先给出一个输入数据，我们的算法会通过一系列的过程得到一个估计的函数，这个函数有能力对没有见过的新数据给出一个新的估计，也被称为构建一个模型。就如同上面的线性回归函数。</p>
<p><img src="../../../static/images/mllr0.png" alt="pic" /></p>
<p>我们用X1，X2..Xn 去描述feature里面的分量，比如x1=房间的面积，x2=房间的朝向，等等，我们可以做出一个估计函数：</p>
<p><img src="../../../static/images/mllr1.png" alt="pic" /></p>
<p>θ在这儿称为参数，在这儿的意思是调整feature中每个分量的影响力，就是到底是房屋的面积更重要还是房屋的地段更重要。为了如果我们令X0 = 1，就可以用向量的方式来表示了：</p>
<p><img src="../../../static/images/mllr2.png" alt="pic" /></p>
<p>我们程序也需要一个机制去评估我们θ是否比较好，所以说需要对我们做出的h函数进行评估，一般这个函数称为损失函数（loss function）或者错误函数(error function)，描述h函数<strong>不好</strong>的程度，在下面，我们称这个函数为J函数</p>
<p>在这儿我们可以做出下面的一个错误函数：</p>
<p><img src="../../../static/images/mllr3.png" alt="pic" /></p>
<p>这个错误估计函数是去对x(i)的估计&#20540;与真实&#20540;y(i)差的平方和作为错误估计函数，前面乘上的1/2是为了在求导的时候，这个系数就不见了。</p>
<p>如何调整θ以使得J(θ)取得最小&#20540;有很多方法，其中有最小二乘法(min square)，是一种完全是数学描述的方法，在stanford机器学习开放课最后的部分会推导最小二乘法的公式的来源，这个来很多的机器学习和数学书 上都可以找到，这里就不提最小二乘法，而谈谈梯度下降法。</p>
<p>梯度下降法是按下面的流程进行的：</p>
<p>1）首先对θ赋&#20540;，这个&#20540;可以是随机的，也可以让θ是一个全零的向量。</p>
<p>2）改变θ的&#20540;，使得J(θ)按梯度下降的方向进行减少。</p>
<p>为了更清楚，给出下面的图：</p>
<p><img src="../../../static/images/mllr4.png" alt="pic" /></p>
<p>这是一个表示参数θ与误差函数J(θ)的关系图，红色的部分是表示J(θ)有着比较高的取&#20540;，我们需要的是，能够让J(θ)的&#20540;尽量的低。也就是深蓝色的部分。θ0，θ1表示θ向量的两个维度。</p>
<p>在上面提到梯度下降法的第一步是给θ给一个初&#20540;，假设随机给的初&#20540;是在图上的十字点。</p>
<p>然后我们将θ按照梯度下降的方向进行调整，就会使得J(θ)往更低的方向进行变化，如图所示，算法的结束将是在θ下降到无法继续下降为止。</p>
<p><img src="../../../static/images/mllr5.png" alt="pic" /></p>

<p>下面我将用一个例子描述一下梯度减少的过程，对于我们的函数J(θ)求偏导J：（求导的过程如果不明白，可以温习一下微积分）</p>
<p><img src="../../../static/images/mllr6.png" alt="pic" /></p>
<p>下面是更新的过程，也就是θi会向着梯度最小的方向进行减少。θi表示更新之前的&#20540;，-后面的部分表示按梯度方向减少的量，α表示步长，也就是每次按照梯度减少的方向变化多少。</p>
<p><img src="../../../static/images/mllr7.png" alt="pic" /></p>
<p>一个很重要的地方&#20540;得注意的是，梯度是有方向的，对于一个向量θ，每一维分量θi都可以求出一个梯度的方向，我们就可以找到一个整体的方向，在变化的时候，我们就朝着下降最多的方向进行变化就可以达到一个最小点，不管它是局部的还是全局的。</p>

<p><strong>2.最小二乘法</strong></p>
<p>是一个直接的数学求解公式，不过它要求X是列满秩的，</p>
<p><img src="../../../static/images/mllr8.png" alt="pic" /></p>

<p>X^TX，这两个矩阵相乘的结果，一个p*p的矩阵(p是解释变量的个数)，一定是可逆的吗？不可逆的话刚才算beta的公式不就没有意义了吗？</p>

<p>幸运的是，在实际生活中，只要我们的数据真的是随机抽取的，这个矩阵一般都是可逆的。
不幸的是，有一种存在叫做almost singular。
一个方阵(行数=列数, 比如上面那个p*p矩阵)一定可以被归类到以下两种情况：</p>

<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>singular: 行列式</td>
          <td>X</td>
          <td>=0，特征根中至少有一个是0，不满秩，不可以求逆。</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>nonsingular: 行列式</td>
          <td>X</td>
          <td>不等于0，特征根都不等于0，满秩，可以求逆。</td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

<p>详见知乎回答：
<a href="http://www.zhihu.com/question/38121173/answer/75307914">Linear least squares, Lasso,ridge regression有何本质区别？</a></p>

<h2 id="sparkml">利用SparkML求解</h2>
<p>测试数据格式为libsvm(y 参数号:x)：</p>

<p>1 1:1</p>

<p>2 1:2</p>

<p>3 1:4</p>

<p>4 1:3</p>

<div class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">linearTraining</span> <span class="k">=</span> <span class="n">ss</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">format</span><span class="o">(</span><span class="s">"libsvm"</span><span class="o">)</span>
      <span class="o">.</span><span class="n">load</span><span class="o">(</span><span class="s">"/home/pi/doc/Spark/spark-2.0.0-bin-hadoop2.7/data/mllib/test_linear_regression_data.txt"</span><span class="o">)</span>

    <span class="c1">//ordinary least squares or linear least squares uses no regularization
</span>    <span class="k">val</span> <span class="n">linearRegression</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">LinearRegression</span><span class="o">()</span>
      <span class="o">.</span><span class="n">setMaxIter</span><span class="o">(</span><span class="mi">100</span><span class="o">)</span>
    <span class="c1">//正则化参数
</span>      <span class="c1">//.setRegParam(0.5)
</span>    <span class="cm">/**
      * Param for the ElasticNet mixing parameter, in range [0, 1].
      * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.
      * ridge regression uses L2 regularization; and Lasso uses L1 regularization.
      */</span>
      <span class="c1">//.setElasticNetParam(0)
</span>
    <span class="c1">// Fit the model
</span>    <span class="k">val</span> <span class="n">lrModel</span> <span class="k">=</span> <span class="n">linearRegression</span><span class="o">.</span><span class="n">fit</span><span class="o">(</span><span class="n">linearTraining</span><span class="o">)</span>

    <span class="c1">// Print the coefficients and intercept for linear regression
</span>    <span class="n">println</span><span class="o">(</span><span class="n">s</span><span class="s">"Coefficients: ${lrModel.coefficients} Intercept: ${lrModel.intercept}"</span><span class="o">)</span>

    <span class="c1">// Summarize the model over the training set and print out some metrics
</span>    <span class="k">val</span> <span class="n">trainingSummary</span> <span class="k">=</span> <span class="n">lrModel</span><span class="o">.</span><span class="n">summary</span>
    <span class="n">println</span><span class="o">(</span><span class="n">s</span><span class="s">"numIterations: ${trainingSummary.totalIterations}"</span><span class="o">)</span>
    <span class="n">println</span><span class="o">(</span><span class="n">s</span><span class="s">"objectiveHistory: ${trainingSummary.objectiveHistory.toList}"</span><span class="o">)</span>
    <span class="n">trainingSummary</span><span class="o">.</span><span class="n">residuals</span><span class="o">.</span><span class="n">show</span><span class="o">()</span>
    <span class="n">println</span><span class="o">(</span><span class="n">s</span><span class="s">"RMSE: ${trainingSummary.rootMeanSquaredError}"</span><span class="o">)</span>
    <span class="n">println</span><span class="o">(</span><span class="n">s</span><span class="s">"r2: ${trainingSummary.r2}"</span><span class="o">)</span></code></pre></div>

<p>测试结果：</p>

<p>Coefficients: [0.7999999999999996] Intercept: 0.500000000000001<br />
numIterations: 1<br />
objectiveHistory: List(0.0)<br />
+——————–+<br />
|           residuals|<br />
+——————–+<br />
| -0.3000000000000007|<br />
|-0.10000000000000009|<br />
| -0.6999999999999993|<br />
|  1.1000000000000005|<br />
+——————–+<br /></p>

<p>RMSE: 0.670820393249937<br />
r2: 0.6399999999999999</p>


</article>
<p class="lotus-anno">声明: 本文采用 <a href="http://creativecommons.org/licenses/by-nc-sa/3.0/" rel="nofollow" target="_blank" title="自由转载-非商用-非衍生-保持署名">BY-NC-SA</a> 授权。转载请注明转自: <a href="" title="" rel="nofollow">phy</a></p>
<section class="lotus-nextpage fn-clear">
    
    <div class="lotus-nextpage-left"><a class="prev" href="/2016/08/11/sparkstandalone.html" rel="prev">&laquo;&nbsp;Spark集群搭建</a></div>
    
    
    <div class="lotus-nextpage-right"><a class="next" href="/2016/09/03/logisticregression.html" rel="next">机器学习算法之逻辑回归&nbsp;&raquo;</a></div>
    
</section>

<!-- 多说评论框 start -->
	<div class="ds-thread" data-thread-key="/2016/08/25/mllinearregression" data-title="机器学习算法之线性回归" data-url="/2016/08/25/mllinearregression.html"></div>
<!-- 多说评论框 end -->
<!-- 多说公共JS代码 start (一个网页只需插入一次) -->
<script type="text/javascript">
var duoshuoQuery = {short_name:"pp1230"};
	(function() {
		var ds = document.createElement('script');
		ds.type = 'text/javascript';ds.async = true;
		ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
		ds.charset = 'UTF-8';
		(document.getElementsByTagName('head')[0] 
		 || document.getElementsByTagName('body')[0]).appendChild(ds);
	})();
	</script>
<!-- 多说公共JS代码 end -->




<footer class="lotus-footer">
	<p>Copyright © 2010–2015 PHY的博客 All rights reserved. On github <a href="https://github.com/pp1230" target="_blank">pp1230</a>.</p>
</footer>
<script src="http://pp1230.github.io/static/js/jquery.scrollTo.js" type="text/javascript"></script>
<script src="http://pp1230.github.io/static/js/iLotus.js" type="text/javascript"></script>
</body>
</html>
