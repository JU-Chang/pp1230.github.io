<!DOCTYPE html>
<html id="J-html" class="">
<head>
    <meta charset="UTF-8" />
    <title>
        
            Spark Api
        
    </title>
    <meta name="generator" content="Jekyll" />
    <meta name="author" content="phy" />
    <meta name="description" content="Spark Api 学习" />
    <meta name="keywords" content="Spark,rdd" />
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
   <link rel="stylesheet" type="text/css" media="all" href="../../../static/style.css" />
    <link rel="stylesheet" type="text/css" media="all" href="../../../static/pygments.css" />

    <!--[if lt IE 9]>
    <script src="http://pp1230.github.io/static/js/html5.js" type="text/javascript"></script>
    <![endif]-->
    <script src="http://pp1230.github.io/static/js/jquery.js" type="text/javascript"></script>
<script type="text/javascript"
 src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</head>
<body itemscope itemtype="http://schema.org/WebPage" class="home blog lotus index">
    <nav class="lotus-nav">
        <ul>
            
            
            
            
            
                
            
            <li class="home ">
                <a href="/index.html" rel="bookmark" title="首页">
                    <i class="icon-home"></i>
                </a>
                
            </li>
            
            
            
            
            
            <li class="">
                <a href="/archives.html" rel="bookmark" title="文章归档">
                    <i class="icon-reorder"></i>
                </a>
                
            </li>
            
            
            
            
            
            <li class="">
                <a href="/contact.html" rel="bookmark" title="关于我">
                    <i class="icon-envelope-alt"></i>
                </a>
                
            </li>
            
            
            
            
            
            <li class="">
                <a href="/love/pwd.html" rel="bookmark" title="love">
                    <i class="icon-heart"></i>
                </a>
                
            </li>
            
            
            
            
            
            <li class="">
                <a href="/resource.html" rel="bookmark" title="资源">
                    <i class="icon-github"></i>
                </a>
                
            </li>
            
        </ul>
    </nav>

    <p class="lotus-breadcrub">
    <a href="http://pp1230.github.io/index.html" rel="nofollow" rel="nofollow" title="首页">Home</a>
    <span> &gt; </span>
    <a href="http://pp1230.github.io/archives.html" rel="nofollow" >Archives</a>
    <span> &gt; </span>
    Spark Api
</p>
<h1 class="lotus-pagetit">Spark Api</h1>
<p class="lotus-meta">Publish: <time class="date" pubdate="July 25, 2016">July 25, 2016</time></p>
<article  itemscope itemtype="http://schema.org/Article" class="lotus-post">
<h1 id="spark-api">SPARK API</h1>

<blockquote>
  <p>Spark Api Docs : <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.package">http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.package</a></p>
</blockquote>

<h2 id="sparkcontext">SparkContext</h2>

<ul>
  <li>
    <p>def
<strong>parallelize[T] (seq: Seq[T], numSlices: Int = defaultParallelism(分组参数，可以为空))(implicit arg0: ClassTag[T]): RDD[T]</strong></p>

    <p>Distribute a local Scala collection to form an RDD.</p>

    <p><strong>Note</strong>:
avoid using parallelize(Seq()) to create an empty RDD. Consider emptyRDD for an RDD with no partitions, or parallelize(Seq<a href="">T</a>) for an RDD of T with empty partitions.
,
Parallelize acts lazily. If seq is a mutable collection and is altered after the call to parallelize and before the first action on the RDD, the resultant RDD will reflect the modified collection. Pass a copy of the argument to avoid this.</p>
  </li>
  <li>
    <p>def
<strong>textFile(path: String, minPartitions: Int = defaultMinPartitions): RDD[String]</strong>
  Read a text file from HDFS, a local file system (available on all nodes), or any Hadoop-supported file system URI, and return it as an RDD of Strings.</p>
  </li>
</ul>

<div class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="c1">//从本地文件初始化RDD
</span><span class="n">scala</span><span class="o">&gt;</span> <span class="k">val</span> <span class="n">rdd2</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="o">(</span><span class="s">"data.txt"</span><span class="o">)</span>
<span class="n">rdd2</span><span class="k">:</span> <span class="kt">org.apache.spark.rdd.RDD</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="n">data</span><span class="o">.</span><span class="n">txt</span> <span class="nc">MapPartitionsRDD</span><span class="o">[</span><span class="err">3</span><span class="o">]</span> <span class="n">at</span> <span class="n">textFile</span> <span class="n">at</span> <span class="o">&lt;</span><span class="n">console</span><span class="k">&gt;:</span><span class="mi">27</span>

<span class="n">scala</span><span class="o">&gt;</span> <span class="n">rdd2</span><span class="o">.</span><span class="n">toArray</span>
<span class="n">res2</span><span class="k">:</span> <span class="kt">Array</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="nc">Array</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="mi">11</span><span class="o">,</span> <span class="mi">222</span><span class="o">,</span> <span class="mi">3333</span><span class="o">,</span> <span class="mi">44444</span><span class="o">)</span></code></pre></div>

<h2 id="rdd">RDD</h2>

<p>A Resilient Distributed Dataset (RDD), the basic abstraction in Spark. Represents an immutable, partitioned collection of elements that can be operated on in parallel. This class contains the basic operations available on all RDDs, such as map, filter, and persist. In addition, org.apache.spark.rdd.<strong>PairRDDFunctions</strong> contains operations available only on RDDs of key-value pairs, such as groupByKey and join; org.apache.spark.rdd.<strong>DoubleRDDFunctions</strong> contains operations available only on RDDs of Doubles; and org.apache.spark.rdd.<strong>SequenceFileRDDFunctions</strong> contains operations available on RDDs that can be saved as SequenceFiles. All operations are automatically available on any RDD of the right type (e.g. RDD[(Int, Int)] through implicit.</p>

<p>RDD是Spark的基本抽象类，包含了可以分布式运算的集合元素。RDD类包含了基本的map filtet persist方法，当RDD是键值对和Double时有额外的方法。</p>

<ul>
  <li>
    <p>def
<strong>++(other: RDD[T]): RDD[T]</strong></p>

    <p>Return the union of this RDD and another one(合并两个RDD). Any identical elements will appear multiple times (use .distinct() to eliminate them).</p>
  </li>
  <li>
    <p>def
<strong>aggregate[U] (zeroValue : U)(seqOp: (U, T) ⇒ U, combOp: (U, U) ⇒ U)(implicit arg0: ClassTag[U]): U</strong></p>

    <p>Aggregate the elements of each partition, and then the results for all the partitions, using given combine functions and a neutral “zero value”. This function can return a different result type, U, than the type of this RDD, T. Thus, we need one operation for merging a T into an U and one operation for merging two U’s, as in scala.TraversableOnce. Both of these functions are allowed to modify and return their first argument instead of creating a new U to avoid memory allocation.</p>

    <p><strong>zeroValue</strong>
the initial value for the accumulated result of each partition for the seqOp operator, and also the initial value for the combine results from different partitions for the combOp operator - this will typically be the neutral element (e.g. Nil for list concatenation or 0 for summation)</p>

    <p><strong>seqOp</strong>（聚合函数）
an operator used to accumulate results within a partition</p>

    <p><strong>combOp</strong>（统计函数）
an associative operator used to combine results from different partitions</p>
  </li>
</ul>

<div class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="n">scala</span><span class="o">&gt;</span> <span class="k">val</span> <span class="n">z</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span><span class="s">"a"</span><span class="o">,</span><span class="s">"b"</span><span class="o">),</span><span class="mi">2</span><span class="o">)</span>
<span class="n">z</span><span class="k">:</span> <span class="kt">org.apache.spark.rdd.RDD</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="nc">ParallelCollectionRDD</span><span class="o">[</span><span class="err">37</span><span class="o">]</span> <span class="n">at</span> <span class="n">parallelize</span> <span class="n">at</span> <span class="o">&lt;</span><span class="n">console</span><span class="k">&gt;:</span><span class="mi">27</span>

<span class="cm">/*函数执行过程
seq:x+a
com:x+xa
seq:x+b
com:xxa+xb
res69: String = xxaxb
*/</span>

<span class="n">scala</span><span class="o">&gt;</span> <span class="n">z</span><span class="o">.</span><span class="n">aggregate</span><span class="o">(</span><span class="s">"x"</span><span class="o">)(</span><span class="k">_</span> <span class="o">+</span> <span class="k">_</span><span class="o">,</span> <span class="k">_</span><span class="o">+</span><span class="k">_</span><span class="o">)</span>
<span class="n">res69</span><span class="k">:</span> <span class="kt">String</span> <span class="o">=</span> <span class="n">xxbxa</span></code></pre></div>

<ul>
  <li>
    <p>def
<strong>cache(): RDD.this.type</strong></p>

    <p>Persist this RDD with the default storage level (MEMORY_ONLY).</p>
  </li>
  <li>
    <p>def
<strong>cartesian[U] (other: RDD[U])(implicit arg0: ClassTag[U]): RDD[(T, U)]</strong>(两个rdd做笛卡尔积)
  Return the Cartesian product of this RDD and another one, that is, the RDD of all pairs of elements (a, b) where a is in this and b is in other.</p>
  </li>
</ul>

<div class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="c1">//rdd1=1,2,3;rdd3=a,b
</span><span class="n">scala</span><span class="o">&gt;</span> <span class="k">val</span> <span class="n">rdd4</span> <span class="k">=</span> <span class="n">rdd3</span><span class="o">.</span><span class="n">cartesian</span><span class="o">(</span><span class="n">rdd1</span><span class="o">)</span>
<span class="n">rdd4</span><span class="k">:</span> <span class="kt">org.apache.spark.rdd.RDD</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">)]</span> <span class="k">=</span> <span class="nc">CartesianRDD</span><span class="o">[</span><span class="err">6</span><span class="o">]</span> <span class="n">at</span> <span class="n">cartesian</span> <span class="n">at</span> <span class="o">&lt;</span><span class="n">console</span><span class="k">&gt;:</span><span class="mi">33</span>

<span class="n">scala</span><span class="o">&gt;</span> <span class="n">rdd4</span><span class="o">.</span><span class="n">collect</span>
<span class="n">res7</span><span class="k">:</span> <span class="kt">Array</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">)]</span> <span class="k">=</span> <span class="nc">Array</span><span class="o">((</span><span class="n">a</span><span class="o">,</span><span class="mi">1</span><span class="o">),</span> <span class="o">(</span><span class="n">a</span><span class="o">,</span><span class="mi">3</span><span class="o">),</span> <span class="o">(</span><span class="n">a</span><span class="o">,</span><span class="mi">2</span><span class="o">),</span> <span class="o">(</span><span class="n">b</span><span class="o">,</span><span class="mi">1</span><span class="o">),</span> <span class="o">(</span><span class="n">b</span><span class="o">,</span><span class="mi">3</span><span class="o">),</span> <span class="o">(</span><span class="n">b</span><span class="o">,</span><span class="mi">2</span><span class="o">))</span></code></pre></div>

<ul>
  <li>def
<strong>collect[U] (f: PartialFunction<a href="Scala偏函数">T, U</a>)(implicit arg0: ClassTag[U]): RDD[U]</strong>
Return an RDD that contains all matching values by applying f.</li>
</ul>

<div class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="c1">//定义偏函数,大于1的元素+1
</span><span class="n">scala</span><span class="o">&gt;</span> <span class="k">val</span> <span class="n">pfunc</span> <span class="k">:</span> <span class="kt">PartialFunction</span><span class="o">[</span><span class="kt">Int</span>,<span class="kt">Int</span><span class="o">]</span> <span class="k">=</span> <span class="o">{</span> <span class="k">case</span> <span class="n">s</span> <span class="k">if</span> <span class="n">s</span><span class="o">&gt;</span><span class="mi">1</span> <span class="k">=&gt;</span> <span class="n">s</span><span class="o">+</span><span class="mi">1</span> <span class="o">}</span>
<span class="n">pfunc</span><span class="k">:</span> <span class="kt">PartialFunction</span><span class="o">[</span><span class="kt">Int</span>,<span class="kt">Int</span><span class="o">]</span> <span class="k">=</span> <span class="o">&lt;</span><span class="n">function1</span><span class="o">&gt;</span>

<span class="n">scala</span><span class="o">&gt;</span> <span class="n">rdd1</span><span class="o">.</span><span class="n">collect</span>
<span class="n">res29</span><span class="k">:</span> <span class="kt">Array</span><span class="o">[</span><span class="kt">Int</span><span class="o">]</span> <span class="k">=</span> <span class="nc">Array</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="mi">3</span><span class="o">,</span> <span class="mi">2</span><span class="o">)</span>

<span class="n">scala</span><span class="o">&gt;</span> <span class="k">val</span> <span class="n">rdd5</span> <span class="k">=</span> <span class="n">rdd1</span><span class="o">.</span><span class="n">collect</span><span class="o">(</span><span class="n">pfunc</span><span class="o">)</span>
<span class="n">rdd5</span><span class="k">:</span> <span class="kt">org.apache.spark.rdd.RDD</span><span class="o">[</span><span class="kt">Int</span><span class="o">]</span> <span class="k">=</span> <span class="nc">MapPartitionsRDD</span><span class="o">[</span><span class="err">14</span><span class="o">]</span> <span class="n">at</span> <span class="n">collect</span> <span class="n">at</span> <span class="o">&lt;</span><span class="n">console</span><span class="k">&gt;:</span><span class="mi">33</span>

<span class="n">scala</span><span class="o">&gt;</span> <span class="n">rdd5</span><span class="o">.</span><span class="n">collect</span>
<span class="n">res30</span><span class="k">:</span> <span class="kt">Array</span><span class="o">[</span><span class="kt">Int</span><span class="o">]</span> <span class="k">=</span> <span class="nc">Array</span><span class="o">(</span><span class="mi">4</span><span class="o">,</span> <span class="mi">3</span><span class="o">)</span></code></pre></div>

<ul>
  <li>
    <p>def
<strong>distinct(): RDD[T]</strong></p>

    <p>Return a new RDD containing the distinct elements in this RDD.（去重函数）</p>
  </li>
  <li>
    <p>def
<strong>distinct(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T]</strong></p>

    <p>Return a new RDD containing the distinct elements in this RDD.</p>
  </li>
  <li>
    <p>def
<strong>count(): Long</strong></p>

    <p>Return the number of elements in the RDD.(计数函数)</p>
  </li>
  <li>
    <p>def
<strong>countApprox(timeout: Long, confidence: Double = 0.95): PartialResult[BoundedDouble]</strong></p>

    <p>Approximate version of count() that returns a potentially incomplete result within a timeout, even if not all tasks have finished.（计算时间和计算可信度）</p>
  </li>
  <li>
    <p>def
<strong>countApproxDistinct(relativeSD: Double = 0.05): Long</strong></p>

    <p>Return approximate number of distinct elements in the RDD.（统计不同的元素）</p>

    <p>The algorithm used is based on streamlib’s implementation of “HyperLogLog in Practice: Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm”, available here.
relativeSD
Relative accuracy. Smaller values create counters that require more space. It must be greater than 0.000017.</p>
  </li>
</ul>

<div class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="n">scala</span><span class="o">&gt;</span> <span class="k">val</span> <span class="n">data</span> <span class="k">=</span> <span class="nc">Array</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span><span class="mi">1</span><span class="o">,</span><span class="mi">2</span><span class="o">,</span><span class="mi">3</span><span class="o">,</span><span class="mi">21</span><span class="o">)</span>
<span class="n">data</span><span class="k">:</span> <span class="kt">Array</span><span class="o">[</span><span class="kt">Int</span><span class="o">]</span> <span class="k">=</span> <span class="nc">Array</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="mi">1</span><span class="o">,</span> <span class="mi">2</span><span class="o">,</span> <span class="mi">3</span><span class="o">,</span> <span class="mi">21</span><span class="o">)</span>

<span class="n">scala</span><span class="o">&gt;</span> <span class="n">distrdd</span><span class="o">.</span><span class="n">count</span>
<span class="n">res1</span><span class="k">:</span> <span class="kt">Long</span> <span class="o">=</span> <span class="mi">5</span>                                                                  
                      <span class="o">^</span>
<span class="c1">//100毫秒计算时间
</span><span class="n">scala</span><span class="o">&gt;</span> <span class="n">distrdd</span><span class="o">.</span><span class="n">countApprox</span><span class="o">(</span><span class="mi">100</span><span class="o">)</span>
<span class="n">res3</span><span class="k">:</span> <span class="kt">org.apache.spark.partial.PartialResult</span><span class="o">[</span><span class="kt">org.apache.spark.partial.BoundedDouble</span><span class="o">]</span> <span class="k">=</span> <span class="o">(</span><span class="k">final:</span> <span class="err">[5</span><span class="kt">.</span><span class="err">000</span><span class="o">,</span> <span class="mf">5.000</span><span class="err">]</span><span class="o">)</span>

<span class="c1">//不同元素有四个
</span><span class="n">scala</span><span class="o">&gt;</span> <span class="n">distrdd</span><span class="o">.</span><span class="n">countApproxDistinct</span><span class="o">(</span><span class="mf">0.05</span><span class="o">)</span>
<span class="n">res4</span><span class="k">:</span> <span class="kt">Long</span> <span class="o">=</span> <span class="mi">4</span></code></pre></div>

<ul>
  <li>
    <p>def
<strong>filter(f: (T) ⇒ Boolean): RDD[T]</strong></p>

    <p>Return a new RDD containing only the elements that satisfy a predicate.(筛选函数)</p>
  </li>
</ul>

<div class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="n">scala</span><span class="o">&gt;</span> <span class="k">val</span> <span class="n">distrdd2</span> <span class="k">=</span> <span class="n">distrdd</span><span class="o">.</span><span class="n">filter</span><span class="o">(</span><span class="n">s</span> <span class="k">=&gt;</span> <span class="n">s</span><span class="o">/</span><span class="mi">2</span><span class="o">==</span><span class="mi">0</span><span class="o">)</span>
<span class="n">distrdd2</span><span class="k">:</span> <span class="kt">org.apache.spark.rdd.RDD</span><span class="o">[</span><span class="kt">Int</span><span class="o">]</span> <span class="k">=</span> <span class="nc">MapPartitionsRDD</span><span class="o">[</span><span class="err">5</span><span class="o">]</span> <span class="n">at</span> <span class="n">filter</span> <span class="n">at</span> <span class="o">&lt;</span><span class="n">console</span><span class="k">&gt;:</span><span class="mi">31</span></code></pre></div>

<h2 id="pairrddfunctions">PairRDDFunctions</h2>

<p>Extra functions available on RDDs of (key, value) pairs through an implicit conversion.
在键值对中的RDD方法</p>

<ul>
  <li>
    <p>def
<strong>reduceByKey(func: (V, V) ⇒ V): RDD[(K, V)]</strong></p>

    <p>Merge the values for each key using an associative and commutative reduce function. This will also perform the merging locally on each mapper before sending results to a reducer, similarly to a “combiner” in MapReduce. Output will be hash-partitioned with the existing partitioner/ parallelism level.</p>
  </li>
  <li>
    <p>def
<strong>join<a href="other: RDD[(K, W)]">W</a>: RDD[(K, (V, W))]</strong></p>

    <p>Return an RDD containing all pairs of elements with matching keys in this and other. Each pair of elements will be returned as a (k, (v1, v2)) tuple, where (k, v1) is in this and (k, v2) is in other. Performs a hash join across the cluster.</p>
  </li>
</ul>

<div class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="c1">//PairRDDFunctions
</span>    <span class="k">val</span> <span class="n">rdd4</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="o">(</span><span class="nc">Array</span><span class="o">((</span><span class="s">"hello"</span><span class="o">,</span><span class="mi">1</span><span class="o">),(</span><span class="s">"pi"</span><span class="o">,</span><span class="mi">1</span><span class="o">)))</span>
    <span class="k">val</span> <span class="n">data1</span> <span class="k">=</span> <span class="s">"hello world hello pi pi haha"</span>
    <span class="c1">//统计每个单词个数
</span>    <span class="k">val</span> <span class="n">rdd3</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="o">(</span><span class="n">data1</span><span class="o">.</span><span class="n">split</span><span class="o">(</span><span class="s">" "</span><span class="o">))</span>
      <span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">i</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">i</span><span class="o">,</span><span class="mi">1</span><span class="o">)).</span><span class="n">reduceByKey</span><span class="o">(</span><span class="k">_</span><span class="o">+</span><span class="k">_</span><span class="o">)</span>
    <span class="c1">//合并相同单词
</span>    <span class="k">val</span> <span class="n">rdd5</span> <span class="k">=</span> <span class="n">rdd3</span><span class="o">.</span><span class="n">join</span><span class="o">[</span><span class="kt">Int</span><span class="o">](</span><span class="n">rdd4</span><span class="o">)</span>

    <span class="n">rdd3</span><span class="o">.</span><span class="n">foreach</span><span class="o">(</span><span class="n">i</span> <span class="k">=&gt;</span> <span class="n">println</span><span class="o">(</span><span class="n">i</span><span class="o">))</span>
    <span class="n">rdd5</span><span class="o">.</span><span class="n">foreach</span><span class="o">(</span><span class="n">i</span> <span class="k">=&gt;</span> <span class="n">println</span><span class="o">(</span><span class="n">i</span><span class="o">))</span>

    <span class="cm">/**
      * 运行结果1
      * (haha,1)
      * (hello,2)
      * (world,1)
      * (pi,2)
      * 
      * 运行结果2
      * (pi,(2,1))
      * (hello,(2,1))
      */</span></code></pre></div>

<h2 id="api">常用 Api</h2>

<div class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="c1">//Transformations
</span>      <span class="n">println</span><span class="o">(</span><span class="s">"============="</span><span class="o">+</span><span class="s">"Transformations"</span><span class="o">+</span><span class="s">"============"</span><span class="o">)</span>

    <span class="k">val</span> <span class="n">numData</span> <span class="k">=</span> <span class="nc">Array</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span><span class="mi">2</span><span class="o">,</span><span class="mi">2</span><span class="o">,</span><span class="mi">3</span><span class="o">)</span>
    <span class="k">val</span> <span class="n">stringArray</span> <span class="k">=</span> <span class="nc">Array</span><span class="o">(</span><span class="s">"pi"</span><span class="o">,</span><span class="s">"huaiyu"</span><span class="o">,</span><span class="s">"hello"</span><span class="o">,</span><span class="s">"pi"</span><span class="o">)</span>
    <span class="k">val</span> <span class="n">stringData</span> <span class="k">=</span> <span class="s">"pi huai yu hello pi"</span>
    <span class="k">val</span> <span class="n">stringArrayData</span> <span class="k">=</span> <span class="nc">Array</span><span class="o">(</span><span class="s">"hello world pi"</span><span class="o">,</span><span class="s">"pi huai yu"</span><span class="o">)</span>

    <span class="k">val</span> <span class="n">numRdd</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="o">(</span><span class="n">numData</span><span class="o">)</span>
    <span class="k">val</span> <span class="n">stringRdd</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="o">(</span><span class="n">stringData</span><span class="o">)</span>
    <span class="cm">/**
      * map(func)
      * Return a new distributed dataset formed by
      * passing each element of the source through a function func.
      */</span>
    <span class="n">println</span><span class="o">(</span><span class="s">"---------"</span><span class="o">+</span><span class="s">"map"</span><span class="o">+</span><span class="s">"----------"</span><span class="o">)</span>
    <span class="k">val</span> <span class="n">mapRdd</span> <span class="k">=</span> <span class="n">numRdd</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="k">_</span><span class="o">+</span><span class="mi">1</span><span class="o">)</span>
    <span class="n">mapRdd</span><span class="o">.</span><span class="n">foreach</span><span class="o">(</span><span class="n">println</span><span class="o">(</span><span class="k">_</span><span class="o">))</span>

    <span class="cm">/**
      * filter(func)
      * Return a new dataset formed by selecting those
      * elements of the source on which func returns true.
      */</span>
    <span class="n">println</span><span class="o">(</span><span class="s">"---------"</span><span class="o">+</span><span class="s">"filter"</span><span class="o">+</span><span class="s">"----------"</span><span class="o">)</span>
    <span class="k">val</span> <span class="n">filterRdd</span> <span class="k">=</span> <span class="n">numRdd</span><span class="o">.</span><span class="n">filter</span><span class="o">(</span><span class="n">i</span> <span class="k">=&gt;</span> <span class="n">i</span><span class="o">&lt;</span><span class="mi">2</span><span class="o">)</span>
    <span class="n">filterRdd</span><span class="o">.</span><span class="n">foreach</span><span class="o">(</span><span class="n">println</span><span class="o">(</span><span class="k">_</span><span class="o">))</span>

    <span class="cm">/**
      * flatMap(func)
      * Similar to map, but each input item can be
      * mapped to 0 or more output items (so func should return a Seq rather than a single item).
      * flatmap和map的不同在于，flatmap从m映射出n个平行元素，map从n映射出相等n个元素
      */</span>
    <span class="n">println</span><span class="o">(</span><span class="s">"---------"</span><span class="o">+</span><span class="s">"flatMap"</span><span class="o">+</span><span class="s">"----------"</span><span class="o">)</span>
    <span class="k">val</span> <span class="n">flatMapRdd1</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="o">(</span><span class="n">stringArrayData</span><span class="o">).</span><span class="n">flatMap</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">split</span><span class="o">(</span><span class="s">" "</span><span class="o">))</span>
    <span class="n">flatMapRdd1</span><span class="o">.</span><span class="n">foreach</span><span class="o">(</span><span class="n">println</span><span class="o">(</span><span class="k">_</span><span class="o">))</span>
    <span class="n">println</span><span class="o">(</span><span class="s">"---------"</span><span class="o">+</span><span class="s">"map"</span><span class="o">+</span><span class="s">"----------"</span><span class="o">)</span>
    <span class="k">val</span> <span class="n">flatMapRdd2</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="o">(</span><span class="n">stringArrayData</span><span class="o">).</span><span class="n">map</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">split</span><span class="o">(</span><span class="s">" "</span><span class="o">))</span>
    <span class="n">flatMapRdd2</span><span class="o">.</span><span class="n">foreach</span><span class="o">(</span><span class="n">i</span> <span class="k">=&gt;</span> <span class="o">{</span>
      <span class="n">println</span><span class="o">(</span><span class="s">"item"</span><span class="o">)</span>
      <span class="n">i</span><span class="o">.</span><span class="n">foreach</span><span class="o">(</span><span class="n">println</span><span class="o">(</span><span class="k">_</span><span class="o">))</span>
    <span class="o">}</span>
    <span class="o">)</span>

    <span class="cm">/**
      * mapPartitions(func)
      * 该函数和map函数类似，只不过映射函数的参数由RDD中的每一个元素变成了RDD中每一个分区的迭代器。
      * 如果在映射的过程中需要频繁创建额外的对象，使用mapPartitions要比map高效的过。
      * 参数preservesPartitioning表示是否保留父RDD的partitioner分区信息。
      */</span>
    <span class="n">println</span><span class="o">(</span><span class="s">"---------"</span><span class="o">+</span><span class="s">"mapPartitions"</span><span class="o">+</span><span class="s">"----------"</span><span class="o">)</span>
    <span class="k">val</span> <span class="n">mapPartitionsRdd</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="o">(</span><span class="n">stringArrayData</span><span class="o">).</span><span class="n">mapPartitions</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">split</span><span class="o">(</span><span class="s">" "</span><span class="o">)))</span>
    <span class="n">mapPartitionsRdd</span><span class="o">.</span><span class="n">foreach</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">foreach</span><span class="o">(</span><span class="n">println</span><span class="o">(</span><span class="k">_</span><span class="o">)))</span>

    <span class="cm">/**
      * mapPartitionsWithIndex(func)
      * Similar to mapPartitions, but also provides func with an integer value representing the index of the          partition,
      * so func must be of type (Int, Iterator&lt;T&gt;) =&gt; Iterator&lt;U&gt; when running on an RDD of type T.
      */</span>
    <span class="n">println</span><span class="o">(</span><span class="s">"---------"</span><span class="o">+</span><span class="s">"mapPartitionsWithIndex"</span><span class="o">+</span><span class="s">"----------"</span><span class="o">)</span>
    <span class="k">val</span> <span class="n">mapPartitionsWithIndexRdd</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="o">(</span><span class="n">stringArrayData</span><span class="o">,</span><span class="mi">2</span><span class="o">).</span><span class="n">mapPartitionsWithIndex</span><span class="o">((</span><span class="n">a</span><span class="o">,</span><span class="n">b</span><span class="o">)</span> <span class="o">=&gt;{</span>
      <span class="k">if</span><span class="o">(</span><span class="n">a</span> <span class="o">==</span> <span class="mi">1</span><span class="o">)</span> <span class="o">{</span>
        <span class="n">println</span><span class="o">(</span><span class="n">a</span><span class="o">)</span>
        <span class="n">b</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">split</span><span class="o">(</span><span class="s">""</span><span class="o">))</span>
      <span class="o">}</span>
      <span class="k">else</span> <span class="o">{</span>
        <span class="n">println</span><span class="o">(</span><span class="n">a</span><span class="o">)</span>
        <span class="n">b</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">split</span><span class="o">(</span><span class="s">" "</span><span class="o">))</span>
      <span class="o">}</span>
    <span class="o">}</span>
    <span class="o">)</span>
    <span class="n">mapPartitionsWithIndexRdd</span><span class="o">.</span><span class="n">foreach</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">foreach</span><span class="o">(</span><span class="n">println</span><span class="o">(</span><span class="k">_</span><span class="o">)))</span>

    <span class="cm">/**
      * sample(withReplacement, fraction, seed)
      * Sample a fraction fraction of the data, with or without replacement,
      * using a given random number generator seed.
      * 对RDD中的集合内元素进行采样，第一个参数withReplacement是true表示有放回取样，false表示无放回。
      * 第二个参数表示比例，第三个参数是随机种子。
      */</span>
    <span class="n">println</span><span class="o">(</span><span class="s">"---------"</span><span class="o">+</span><span class="s">"sample"</span><span class="o">+</span><span class="s">"----------"</span><span class="o">)</span>
    <span class="k">val</span> <span class="n">sampleRdd</span> <span class="k">=</span> <span class="n">numRdd</span><span class="o">.</span><span class="n">sample</span><span class="o">(</span><span class="kc">false</span><span class="o">,</span><span class="mf">0.5</span><span class="o">)</span>
    <span class="n">sampleRdd</span><span class="o">.</span><span class="n">foreach</span><span class="o">(</span><span class="n">println</span><span class="o">(</span><span class="k">_</span><span class="o">))</span>

    <span class="cm">/**
      *   union(otherDataset)
      * 	Return a new dataset that contains the union of
      * 	the elements in the source dataset and the argument.
      */</span>

    <span class="n">println</span><span class="o">(</span><span class="s">"---------"</span><span class="o">+</span><span class="s">"union"</span><span class="o">+</span><span class="s">"----------"</span><span class="o">)</span>
    <span class="k">val</span> <span class="n">unionRdd</span> <span class="k">=</span> <span class="n">numRdd</span><span class="o">.</span><span class="n">union</span><span class="o">(</span><span class="n">sampleRdd</span><span class="o">)</span>
    <span class="n">unionRdd</span><span class="o">.</span><span class="n">foreach</span><span class="o">(</span><span class="n">println</span><span class="o">(</span><span class="k">_</span><span class="o">))</span>

    <span class="cm">/**
      * intersection(otherDataset)
      * Return a new RDD that contains the intersection of
      * elements in the source dataset and the argument.
      */</span>
    <span class="n">println</span><span class="o">(</span><span class="s">"---------"</span><span class="o">+</span><span class="s">"intersection"</span><span class="o">+</span><span class="s">"----------"</span><span class="o">)</span>
    <span class="k">val</span> <span class="n">intersectionRdd</span> <span class="k">=</span> <span class="n">numRdd</span><span class="o">.</span><span class="n">intersection</span><span class="o">(</span><span class="n">sampleRdd</span><span class="o">)</span>
    <span class="n">intersectionRdd</span><span class="o">.</span><span class="n">foreach</span><span class="o">(</span><span class="n">println</span><span class="o">(</span><span class="k">_</span><span class="o">))</span>

    <span class="cm">/**
      * distinct([numTasks]))
      * Return a new dataset that contains the distinct elements of the source dataset.
      */</span>
    <span class="n">println</span><span class="o">(</span><span class="s">"---------"</span><span class="o">+</span><span class="s">"distinct"</span><span class="o">+</span><span class="s">"----------"</span><span class="o">)</span>
    <span class="k">val</span> <span class="n">distinctRdd</span> <span class="k">=</span> <span class="n">numRdd</span><span class="o">.</span><span class="n">distinct</span><span class="o">()</span>
    <span class="n">distinctRdd</span><span class="o">.</span><span class="n">foreach</span><span class="o">(</span><span class="n">println</span><span class="o">(</span><span class="k">_</span><span class="o">))</span>

    <span class="cm">/**
      *   groupByKey([numTasks])
      * 	When called on a dataset of (K, V) pairs, returns a dataset of (K, Iterable&lt;V&gt;) pairs.
      * 	Note: If you are grouping in order to perform an aggregation (such as a sum or average) over each key,
      * 	using reduceByKey or aggregateByKey will yield much better performance.
      * 	Note: By default, the level of parallelism in the output depends on the number of
      * 	partitions of the parent RDD. You can pass an optional numTasks argument to set a different number of tasks.
      */</span>
    <span class="n">println</span><span class="o">(</span><span class="s">"---------"</span><span class="o">+</span><span class="s">"groupByKey"</span><span class="o">+</span><span class="s">"----------"</span><span class="o">)</span>
    <span class="k">val</span> <span class="n">groupByKeyRdd</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="o">(</span><span class="n">stringArrayData</span><span class="o">).</span><span class="n">flatMap</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">split</span><span class="o">(</span><span class="s">" "</span><span class="o">)).</span><span class="n">map</span><span class="o">((</span><span class="k">_</span><span class="o">,</span><span class="mi">1</span><span class="o">)).</span><span class="n">groupByKey</span><span class="o">()</span>
    <span class="n">groupByKeyRdd</span><span class="o">.</span><span class="n">foreach</span><span class="o">(</span><span class="n">println</span><span class="o">(</span><span class="k">_</span><span class="o">))</span>

    <span class="cm">/**
      *   reduceByKey(func, [numTasks])
      * 	When called on a dataset of (K, V) pairs, returns a dataset of (K, V) pairs
      * 	where the values for each key are aggregated using the given reduce function func,
      * 	which must be of type (V,V) =&gt; V. Like in groupByKey,
      * 	the number of reduce tasks is configurable through an optional second argument.
      */</span>
    <span class="n">println</span><span class="o">(</span><span class="s">"---------"</span><span class="o">+</span><span class="s">"reduceByKey"</span><span class="o">+</span><span class="s">"----------"</span><span class="o">)</span>
    <span class="k">val</span> <span class="n">reduceByKeyRdd</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="o">(</span><span class="n">stringArrayData</span><span class="o">).</span><span class="n">flatMap</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">split</span><span class="o">(</span><span class="s">" "</span><span class="o">)).</span><span class="n">map</span><span class="o">((</span><span class="k">_</span><span class="o">,</span><span class="mi">1</span><span class="o">)).</span><span class="n">reduceByKey</span><span class="o">(</span><span class="k">_</span><span class="o">+</span><span class="k">_</span><span class="o">)</span>
    <span class="n">reduceByKeyRdd</span><span class="o">.</span><span class="n">foreach</span><span class="o">(</span><span class="n">println</span><span class="o">(</span><span class="k">_</span><span class="o">))</span>

    <span class="cm">/**
      * aggregateByKey(zeroValue)(seqOp, combOp, [numTasks])
      * Aggregate the values of each key, using given combine functions and a neutral "zero value".
      * This function can return a different result type, U, than the type of the values in this RDD, V.
      * Thus, we need one operation for merging a V into a U and one operation for merging two U's, as in scala.
      * TraversableOnce. The former operation is used for merging values within a partition,
      * and the latter is used for merging values between partitions. To avoid memory allocation,
      * both of these functions are allowed to modify and return their first argument instead of creating a new U.
      * 聚合键值对的value，通过seqOp方法可以聚合不同类型的value。例如(K,V)，zerovalue类型为U，通过seqOp方法返回U类型的值。
      * 最终通过comOp方法将所有value聚合。
      */</span>
    <span class="n">println</span><span class="o">(</span><span class="s">"---------"</span><span class="o">+</span><span class="s">"aggregateByKey"</span><span class="o">+</span><span class="s">"----------"</span><span class="o">)</span>
    <span class="k">val</span> <span class="n">aggregateByKeyRdd</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="o">(</span><span class="n">stringArrayData</span><span class="o">).</span><span class="n">flatMap</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">split</span><span class="o">(</span><span class="s">" "</span><span class="o">))</span>
      <span class="o">.</span><span class="n">map</span><span class="o">((</span><span class="k">_</span><span class="o">,</span><span class="s">"a"</span><span class="o">)).</span><span class="n">aggregateByKey</span><span class="o">(</span><span class="mi">1</span><span class="o">)((</span><span class="n">a</span><span class="o">,</span><span class="n">b</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">a</span><span class="o">,</span><span class="k">_</span><span class="o">+</span><span class="k">_</span><span class="o">)</span>
    <span class="n">aggregateByKeyRdd</span><span class="o">.</span><span class="n">foreach</span><span class="o">(</span><span class="n">println</span><span class="o">(</span><span class="k">_</span><span class="o">))</span>

    <span class="cm">/**
      * sortByKey([ascending], [numTasks])
      * When called on a dataset of (K, V) pairs where K implements Ordered,
      * returns a dataset of (K, V) pairs sorted by keys in ascending or descending order,
      * as specified in the boolean ascending argument.
      */</span>
    <span class="n">println</span><span class="o">(</span><span class="s">"---------"</span><span class="o">+</span><span class="s">"sortByKey"</span><span class="o">+</span><span class="s">"----------"</span><span class="o">)</span>
    <span class="k">val</span> <span class="n">sortByKeyRdd</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="o">(</span><span class="n">stringArrayData</span><span class="o">,</span><span class="mi">1</span><span class="o">).</span><span class="n">flatMap</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">split</span><span class="o">(</span><span class="s">" "</span><span class="o">))</span>
      <span class="o">.</span><span class="n">map</span><span class="o">((</span><span class="k">_</span><span class="o">,</span><span class="s">"a"</span><span class="o">)).</span><span class="n">sortByKey</span><span class="o">()</span>
    <span class="n">sortByKeyRdd</span><span class="o">.</span><span class="n">foreach</span><span class="o">(</span><span class="n">println</span><span class="o">(</span><span class="k">_</span><span class="o">))</span>

    <span class="cm">/**
      * join(otherDataset, [numTasks])
      * When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (V, W)) pairs
      * with all pairs of elements for each key. Outer joins are supported through leftOuterJoin,
      * rightOuterJoin, and fullOuterJoin.
      */</span>
    <span class="n">println</span><span class="o">(</span><span class="s">"---------"</span><span class="o">+</span><span class="s">"join"</span><span class="o">+</span><span class="s">"----------"</span><span class="o">)</span>
    <span class="k">val</span> <span class="n">stringDataTestJoin</span> <span class="k">=</span> <span class="s">"pi huai yu"</span>
    <span class="k">val</span> <span class="n">joinRdd1</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="o">(</span><span class="n">stringArrayData</span><span class="o">).</span><span class="n">flatMap</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">split</span><span class="o">(</span><span class="s">" "</span><span class="o">))</span>
      <span class="o">.</span><span class="n">map</span><span class="o">((</span><span class="k">_</span><span class="o">,</span><span class="s">"a"</span><span class="o">))</span>
    <span class="k">val</span> <span class="n">joinRdd2</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="o">(</span><span class="n">stringDataTestJoin</span><span class="o">.</span><span class="n">split</span><span class="o">(</span><span class="s">" "</span><span class="o">)).</span><span class="n">map</span><span class="o">((</span><span class="k">_</span><span class="o">,</span><span class="mi">1</span><span class="o">))</span>
    <span class="k">val</span> <span class="n">joinRdd3</span> <span class="k">=</span> <span class="n">joinRdd1</span><span class="o">.</span><span class="n">join</span><span class="o">(</span><span class="n">joinRdd2</span><span class="o">)</span>
    <span class="n">joinRdd3</span><span class="o">.</span><span class="n">foreach</span><span class="o">(</span><span class="n">println</span><span class="o">(</span><span class="k">_</span><span class="o">))</span>

    <span class="cm">/**
      *   cogroup(otherDataset, [numTasks])
      * 	When called on datasets of type (K, V) and (K, W), returns a dataset of
      * 	(K, (Iterable&lt;V&gt;, Iterable&lt;W&gt;)) tuples. This operation is also called groupWith.
      *   相比于join，cogroup会将Key相同的元素全部聚合
      */</span>

    <span class="n">println</span><span class="o">(</span><span class="s">"---------"</span><span class="o">+</span><span class="s">"cogroup"</span><span class="o">+</span><span class="s">"----------"</span><span class="o">)</span>
    <span class="k">val</span> <span class="n">cogroupRdd</span> <span class="k">=</span> <span class="n">joinRdd1</span><span class="o">.</span><span class="n">cogroup</span><span class="o">(</span><span class="n">joinRdd2</span><span class="o">)</span>
    <span class="n">cogroupRdd</span><span class="o">.</span><span class="n">foreach</span><span class="o">(</span><span class="n">println</span><span class="o">(</span><span class="k">_</span><span class="o">))</span>

    <span class="cm">/**
      * cartesian(otherDataset)
      * When called on datasets of types T and U, returns a dataset of (T, U) pairs (all pairs of elements).
      */</span>
    <span class="n">println</span><span class="o">(</span><span class="s">"---------"</span><span class="o">+</span><span class="s">"cartesian"</span><span class="o">+</span><span class="s">"----------"</span><span class="o">)</span>
    <span class="k">val</span> <span class="n">cartesianRdd</span> <span class="k">=</span> <span class="n">stringRdd</span><span class="o">.</span><span class="n">cartesian</span><span class="o">(</span><span class="n">numRdd</span><span class="o">)</span>
    <span class="n">cartesianRdd</span><span class="o">.</span><span class="n">foreach</span><span class="o">(</span><span class="n">println</span><span class="o">(</span><span class="k">_</span><span class="o">))</span>

    <span class="cm">/**
      * pipe(command, [envVars])
      * Pipe each partition of the RDD through a shell command,
      * e.g. a Perl or bash script. RDD elements are written to the process's stdin and lines output to
      * its stdout are returned as an RDD of strings.
      * 如何使用？
      */</span>

    <span class="cm">/**
      * coalesce(numPartitions)
      * Decrease the number of partitions in the RDD to numPartitions.
      * Useful for running operations more efficiently after filtering down a large dataset.
      */</span>
    <span class="n">println</span><span class="o">(</span><span class="s">"---------"</span><span class="o">+</span><span class="s">"coalesce"</span><span class="o">+</span><span class="s">"----------"</span><span class="o">)</span>
    <span class="k">val</span> <span class="n">coalesceRdd</span> <span class="k">=</span> <span class="n">numRdd</span><span class="o">.</span><span class="n">filter</span><span class="o">(</span><span class="n">i</span> <span class="k">=&gt;</span> <span class="n">i</span><span class="o">&lt;</span><span class="mi">2</span><span class="o">).</span><span class="n">coalesce</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>
    <span class="n">coalesceRdd</span><span class="o">.</span><span class="n">foreach</span><span class="o">(</span><span class="n">println</span><span class="o">(</span><span class="k">_</span><span class="o">))</span>

    <span class="cm">/**
      *   repartition(numPartitions)
      * 	Reshuffle the data in the RDD randomly to create either more or fewer partitions and balance it across them.
      * 	This always shuffles all data over the network.
      */</span>
    <span class="n">println</span><span class="o">(</span><span class="s">"---------"</span><span class="o">+</span><span class="s">"repartition"</span><span class="o">+</span><span class="s">"----------"</span><span class="o">)</span>
    <span class="k">val</span> <span class="n">repatitionRdd</span> <span class="k">=</span> <span class="n">numRdd</span><span class="o">.</span><span class="n">repartition</span><span class="o">(</span><span class="mi">2</span><span class="o">)</span>
    <span class="n">repatitionRdd</span><span class="o">.</span><span class="n">foreach</span><span class="o">(</span><span class="n">println</span><span class="o">(</span><span class="k">_</span><span class="o">))</span>

    <span class="cm">/**
      * repartitionAndSortWithinPartitions(partitioner)
      * Repartition the RDD according to the given partitioner and, within each resulting partition,
      * sort records by their keys. This is more efficient than calling repartition and then
      * sorting within each partition because it can push the sorting down into the shuffle machinery.
      * api中没看见？
      */</span>

    <span class="c1">//Actions
</span>
    <span class="n">println</span><span class="o">(</span><span class="s">"============="</span><span class="o">+</span><span class="s">"Actions"</span><span class="o">+</span><span class="s">"============"</span><span class="o">)</span>
    <span class="n">println</span><span class="o">(</span><span class="s">"---------"</span><span class="o">+</span><span class="s">"reduce"</span><span class="o">+</span><span class="s">"----------"</span><span class="o">)</span>
    <span class="k">val</span> <span class="n">reduceArray</span> <span class="k">=</span> <span class="n">numRdd</span><span class="o">.</span><span class="n">reduce</span><span class="o">(</span><span class="k">_</span><span class="o">+</span><span class="k">_</span><span class="o">)</span>
    <span class="n">println</span><span class="o">(</span><span class="n">reduceArray</span><span class="o">)</span>

    <span class="n">println</span><span class="o">(</span><span class="s">"---------"</span><span class="o">+</span><span class="s">"count"</span><span class="o">+</span><span class="s">"----------"</span><span class="o">)</span>
    <span class="k">val</span> <span class="n">count</span> <span class="k">=</span> <span class="n">numRdd</span><span class="o">.</span><span class="n">count</span><span class="o">()</span>
    <span class="n">println</span><span class="o">(</span><span class="n">count</span><span class="o">)</span>

    <span class="n">println</span><span class="o">(</span><span class="s">"---------"</span><span class="o">+</span><span class="s">"first"</span><span class="o">+</span><span class="s">"----------"</span><span class="o">)</span>
    <span class="n">println</span><span class="o">(</span><span class="n">numRdd</span><span class="o">.</span><span class="n">first</span><span class="o">())</span>

    <span class="n">println</span><span class="o">(</span><span class="s">"---------"</span><span class="o">+</span><span class="s">"take"</span><span class="o">+</span><span class="s">"----------"</span><span class="o">)</span>
    <span class="n">numRdd</span><span class="o">.</span><span class="n">take</span><span class="o">(</span><span class="mi">2</span><span class="o">).</span><span class="n">foreach</span><span class="o">(</span><span class="n">println</span><span class="o">(</span><span class="k">_</span><span class="o">))</span>

    <span class="cm">/**
      * takeSample(withReplacement, num, [seed])
      * Return an array with a random sample of num elements of the dataset,
      * with or without replacement, optionally pre-specifying a random number generator seed.
      */</span>

    <span class="n">println</span><span class="o">(</span><span class="s">"---------"</span><span class="o">+</span><span class="s">"takeSample"</span><span class="o">+</span><span class="s">"----------"</span><span class="o">)</span>
    <span class="n">numRdd</span><span class="o">.</span><span class="n">takeSample</span><span class="o">(</span><span class="kc">false</span><span class="o">,</span><span class="mi">2</span><span class="o">).</span><span class="n">foreach</span><span class="o">(</span><span class="n">println</span><span class="o">(</span><span class="k">_</span><span class="o">))</span>

    <span class="cm">/**
      * takeOrdered(n, [ordering])
      * Return the first n elements of the RDD using either their natural order or a custom comparator.
      */</span>
    <span class="n">println</span><span class="o">(</span><span class="s">"---------"</span><span class="o">+</span><span class="s">"takeOrdered"</span><span class="o">+</span><span class="s">"----------"</span><span class="o">)</span>
    <span class="n">numRdd</span><span class="o">.</span><span class="n">takeOrdered</span><span class="o">(</span><span class="mi">2</span><span class="o">).</span><span class="n">foreach</span><span class="o">(</span><span class="n">println</span><span class="o">(</span><span class="k">_</span><span class="o">))</span>

<span class="c1">//    println("---------"+"saveAsTextFile"+"----------")
//    numRdd.saveAsTextFile("/home/pi/doc/sparkout/TextFile")
//
//    println("---------"+"saveAsObjectFile"+"----------")
//    numRdd.saveAsObjectFile("/home/pi/doc/sparkout/ObjectFile")
//
//    println("---------"+"saveAsSequenceFile"+"----------")
//    sc.parallelize(stringData.split(" ")).map((_,1)).saveAsSequenceFile("/home/pi/doc/sparkout/SequenceFile")
</span>
    <span class="n">println</span><span class="o">(</span><span class="s">"---------"</span><span class="o">+</span><span class="s">"countByKey"</span><span class="o">+</span><span class="s">"----------"</span><span class="o">)</span>
    <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="o">(</span><span class="n">stringData</span><span class="o">.</span><span class="n">split</span><span class="o">(</span><span class="s">" "</span><span class="o">)).</span><span class="n">map</span><span class="o">((</span><span class="k">_</span><span class="o">,</span><span class="mi">1</span><span class="o">)).</span><span class="n">countByKey</span><span class="o">().</span><span class="n">foreach</span><span class="o">(</span><span class="n">println</span><span class="o">(</span><span class="k">_</span><span class="o">))</span></code></pre></div>

</article>
<p class="lotus-anno">声明: 本文采用 <a href="http://creativecommons.org/licenses/by-nc-sa/3.0/" rel="nofollow" target="_blank" title="自由转载-非商用-非衍生-保持署名">BY-NC-SA</a> 授权。转载请注明转自: <a href="" title="" rel="nofollow">phy</a></p>
<section class="lotus-nextpage fn-clear">
    
    <div class="lotus-nextpage-left"><a class="prev" href="/2016/07/19/clusteredindex.html" rel="prev">&laquo;&nbsp;聚簇索引和非聚簇索引</a></div>
    
    
    <div class="lotus-nextpage-right"><a class="next" href="/2016/07/26/javalambda.html" rel="next">Java Lambda&nbsp;&raquo;</a></div>
    
</section>

<!-- 多说评论框 start -->
	<div class="ds-thread" data-thread-key="/2016/07/25/sparkapi" data-title="Spark Api" data-url="/2016/07/25/sparkapi.html"></div>
<!-- 多说评论框 end -->
<!-- 多说公共JS代码 start (一个网页只需插入一次) -->
<script type="text/javascript">
var duoshuoQuery = {short_name:"pp1230"};
	(function() {
		var ds = document.createElement('script');
		ds.type = 'text/javascript';ds.async = true;
		ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
		ds.charset = 'UTF-8';
		(document.getElementsByTagName('head')[0] 
		 || document.getElementsByTagName('body')[0]).appendChild(ds);
	})();
	</script>
<!-- 多说公共JS代码 end -->




<footer class="lotus-footer">
	<p>Copyright © 2010–2015 PHY的博客 All rights reserved. On github <a href="https://github.com/pp1230" target="_blank">pp1230</a>.</p>
</footer>
<script src="http://pp1230.github.io/static/js/jquery.scrollTo.js" type="text/javascript"></script>
<script src="http://pp1230.github.io/static/js/iLotus.js" type="text/javascript"></script>
</body>
</html>
